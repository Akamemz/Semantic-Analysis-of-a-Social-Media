{
    "topic": "technology",
    "source": "The Guardian",
    "bias": 0,
    "url": "https://www.theguardian.com/news/2018/mar/20/facebook-data-cambridge-analytica-sandy-parakilas",
    "title": "'Utterly horrifying': ex-Facebook insider says covert data harvesting was routine",
    "date": "2018-03-20",
    "authors": "Paul Lewis",
    "content": "Sandy Parakilas says numerous companies deployed these techniques \u2013 likely affecting hundreds of millions of users \u2013 and that Facebook looked the other way\nHundreds of millions of Facebook users are likely to have had their private information harvested by companies that exploited the same terms as the firm that collected data and passed it on to Cambridge Analytica , according to a new whistleblower .\nSandy Parakilas , the platform operations manager at Facebook responsible for policing data breaches by third-party software developers between 2011 and 2012 , told \u2588\u2588\u2588 he warned senior executives at the company that its lax approach to data protection risked a major breach .\nWhere 's Zuck ? Facebook CEO silent as data harvesting scandal unfolds Read more\n\u201c My concerns were that all of the data that left Facebook servers to developers could not be monitored by Facebook , so we had no idea what developers were doing with the data , \u201d he said .\nParakilas said Facebook had terms of service and settings that \u201c people didn \u2019 t read or understand \u201d and the company did not use its enforcement mechanisms , including audits of external developers , to ensure data was not being misused .\nParakilas , whose job was to investigate data breaches by developers similar to the one later suspected of Global Science Research , which harvested tens of millions of Facebook profiles and provided the data to Cambridge Analytica , said the slew of recent disclosures had left him disappointed with his superiors for not heeding his warnings .\n\u201c It has been painful watching , \u201d he said , \u201c because I know that they could have prevented it . \u201d\nPlay Video 3:41 What is the Cambridge Analytica scandal ? - video explainer\nAsked what kind of control Facebook had over the data given to outside developers , he replied : \u201c Zero . Absolutely none . Once the data left Facebook servers there was not any control , and there was no insight into what was going on . \u201d\nParakilas said he \u201c always assumed there was something of a black market \u201d for Facebook data that had been passed to external developers . However , he said that when he told other executives the company should proactively \u201c audit developers directly and see what \u2019 s going on with the data \u201d he was discouraged from the approach .\nHe said one Facebook executive advised him against looking too deeply at how the data was being used , warning him : \u201c Do you really want to see what you \u2019 ll find ? \u201d Parakilas said he interpreted the comment to mean that \u201c Facebook was in a stronger legal position if it didn \u2019 t know about the abuse that was happening \u201d .\nHe added : \u201c They felt that it was better not to know . I found that utterly shocking and horrifying . \u201d\nParakilas first went public with his concerns about privacy at Facebook four months ago , but his direct experience policing Facebook data given to third parties throws new light on revelations over how such data was obtained by Cambridge Analytica .\nFacebook did not respond to a request for comment on the information supplied by Parakilas , but directed \u2588\u2588\u2588 to a November 2017 blogpost in which the company defended its data sharing practices , which it said had \u201c significantly improved \u201d over the last five years .\n\u201c While it \u2019 s fair to criticise how we enforced our developer policies more than five years ago , it \u2019 s untrue to suggest we didn \u2019 t or don \u2019 t care about privacy , \u201d that statement said . \u201c The facts tell a different story . \u201d\nParakilas , 38 , who now works as a product manager for Uber , is particularly critical of Facebook \u2019 s previous policy of allowing developers to access the personal data of friends of people who used apps on the platform , without the knowledge or express consent of those friends .\nThat feature , called friends permission , was a boon to outside software developers who , from 2007 onwards , were given permission by Facebook to build quizzes and games \u2013 like the widely popular FarmVille \u2013 that were hosted on the platform .\nThe apps proliferated on Facebook in the years leading up to the company \u2019 s 2012 initial public offering , an era when most users were still accessing the platform via laptops and computers rather than smartphones .\nFacebook took a 30 % cut of payments made through apps , but in return enabled their creators to have access to Facebook user data .\nParakilas does not know how many companies sought friends permission data before such access was terminated around mid-2014 . However , he said he believes tens or maybe even hundreds of thousands of developers may have done so .\nIt has been painful watching , because I know they could have prevented it\nParakilas estimates that \u201c a majority of Facebook users \u201d could have had their data harvested by app developers without their knowledge . The company now has stricter protocols around the degree of access third parties have to data .\nParakilas said that when he worked at Facebook it failed to take full advantage of its enforcement mechanisms , such as a clause that enables the social media giant to audit external developers who misuse its data .\nLegal action against rogue developers or moves to ban them from Facebook were \u201c extremely rare \u201d , he said , adding : \u201c In the time I was there , I didn \u2019 t see them conduct a single audit of a developer \u2019 s systems . \u201d\nFacebook announced on Monday that it had hired a digital forensics firm to conduct an audit of Cambridge Analytica . The decision comes more than two years after Facebook was made aware of the reported data breach .\nDuring the time he was at Facebook , Parakilas said the company was keen to encourage more developers to build apps for its platform and \u201c one of the main ways to get developers interested in building apps was through offering them access to this data \u201d . Shortly after arriving at the company \u2019 s Silicon Valley headquarters he was told that any decision to ban an app required the personal approval of the chief executive , Mark Zuckerberg , although the policy was later relaxed to make it easier to deal with rogue developers .\nWhile the previous policy of giving developers access to Facebook users \u2019 friends \u2019 data was sanctioned in the small print in Facebook \u2019 s terms and conditions , and users could block such data sharing by changing their settings , Parakilas said he believed the policy was problematic .\n\u201c It was well understood in the company that that presented a risk , \u201d he said . \u201c Facebook was giving data of people who had not authorised the app themselves , and was relying on terms of service and settings that people didn \u2019 t read or understand . \u201d\nIt was this feature that was exploited by Global Science Research , and the data provided to Cambridge Analytica in 2014 . GSR was run by the Cambridge University psychologist Aleksandr Kogan , who built an app that was a personality test for Facebook users .\nThe test automatically downloaded the data of friends of people who took the quiz , ostensibly for academic purposes . Cambridge Analytica has denied knowing the data was obtained improperly , and Kogan maintains he did nothing illegal and had a \u201c close working relationship \u201d with Facebook .\nWhile Kogan \u2019 s app only attracted around 270,000 users ( most of whom were paid to take the quiz ) , the company was then able to exploit the friends permission feature to quickly amass data pertaining to more than 50 million Facebook users .\n\u201c Kogan \u2019 s app was one of the very last to have access to friend permissions , \u201d Parakilas said , adding that many other similar apps had been harvesting similar quantities of data for years for commercial purposes . Academic research from 2010 , based on an analysis of 1,800 Facebooks apps , concluded that around 11 % of third-party developers requested data belonging to friends of users .\nIf those figures were extrapolated , tens of thousands of apps , if not more , were likely to have systematically culled \u201c private and personally identifiable \u201d data belonging to hundreds of millions of users , Parakilas said .\nThe ease with which it was possible for anyone with relatively basic coding skills to create apps and start trawling for data was a particular concern , he added .\nParakilas said he was unsure why Facebook stopped allowing developers to access friends data around mid-2014 , roughly two years after he left the company . However , he said he believed one reason may have been that Facebook executives were becoming aware that some of the largest apps were acquiring enormous troves of valuable data .\nHe recalled conversations with executives who were nervous about the commercial value of data being passed to other companies .\n\u201c They were worried that the large app developers were building their own social graphs , meaning they could see all the connections between these people , \u201d he said . \u201c They were worried that they were going to build their own social networks . \u201d\nParakilas said he lobbied internally at Facebook for \u201c a more rigorous approach \u201d to enforcing data protection , but was offered little support . His warnings included a PowerPoint presentation he said he delivered to senior executives in mid-2012 \u201c that included a map of the vulnerabilities for user data on Facebook \u2019 s platform \u201d .\n\u201c I included the protective measures that we had tried to put in place , where we were exposed , and the kinds of bad actors who might do malicious things with the data , \u201d he said . \u201c On the list of bad actors I included foreign state actors and data brokers . \u201d\nFrustrated at the lack of action , Parakilas left Facebook in late 2012 . \u201c I didn \u2019 t feel that the company treated my concerns seriously . I didn \u2019 t speak out publicly for years out of self-interest , to be frank . \u201d\nHow to protect your Facebook privacy \u2013 or delete yourself completely Read more\nThat changed , Parakilas said , when he heard the congressional testimony given by Facebook lawyers to Senate and House investigators in late 2017 about Russia \u2019 s attempt to sway the presidential election . \u201c They treated it like a PR exercise , \u201d he said . \u201c They seemed to be entirely focused on limiting their liability and exposure rather than helping the country address a national security issue . \u201d\nIt was at that point that Parakilas decided to go public with his concerns , writing an opinion article in the New York Times that said Facebook could not be trusted to regulate itself . Since then , Parakilas has become an adviser to the Center for Humane Technology , which is run by Tristan Harris , a former Google employee turned whistleblower on the industry .",
    "content_original": "Sandy Parakilas says numerous companies deployed these techniques \u2013 likely affecting hundreds of millions of users \u2013 and that Facebook looked the other way\n\nHundreds of millions of Facebook users are likely to have had their private information harvested by companies that exploited the same terms as the firm that collected data and passed it on to Cambridge Analytica, according to a new whistleblower.\n\nSandy Parakilas, the platform operations manager at Facebook responsible for policing data breaches by third-party software developers between 2011 and 2012, told the Guardian he warned senior executives at the company that its lax approach to data protection risked a major breach.\n\nWhere's Zuck? Facebook CEO silent as data harvesting scandal unfolds Read more\n\n\u201cMy concerns were that all of the data that left Facebook servers to developers could not be monitored by Facebook, so we had no idea what developers were doing with the data,\u201d he said.\n\nParakilas said Facebook had terms of service and settings that \u201cpeople didn\u2019t read or understand\u201d and the company did not use its enforcement mechanisms, including audits of external developers, to ensure data was not being misused.\n\nParakilas, whose job was to investigate data breaches by developers similar to the one later suspected of Global Science Research, which harvested tens of millions of Facebook profiles and provided the data to Cambridge Analytica, said the slew of recent disclosures had left him disappointed with his superiors for not heeding his warnings.\n\n\u201cIt has been painful watching,\u201d he said, \u201cbecause I know that they could have prevented it.\u201d\n\nPlay Video 3:41 What is the Cambridge Analytica scandal? - video explainer\n\nAsked what kind of control Facebook had over the data given to outside developers, he replied: \u201cZero. Absolutely none. Once the data left Facebook servers there was not any control, and there was no insight into what was going on.\u201d\n\nParakilas said he \u201calways assumed there was something of a black market\u201d for Facebook data that had been passed to external developers. However, he said that when he told other executives the company should proactively \u201caudit developers directly and see what\u2019s going on with the data\u201d he was discouraged from the approach.\n\nHe said one Facebook executive advised him against looking too deeply at how the data was being used, warning him: \u201cDo you really want to see what you\u2019ll find?\u201d Parakilas said he interpreted the comment to mean that \u201cFacebook was in a stronger legal position if it didn\u2019t know about the abuse that was happening\u201d.\n\nHe added: \u201cThey felt that it was better not to know. I found that utterly shocking and horrifying.\u201d\n\nParakilas first went public with his concerns about privacy at Facebook four months ago, but his direct experience policing Facebook data given to third parties throws new light on revelations over how such data was obtained by Cambridge Analytica.\n\nFacebook did not respond to a request for comment on the information supplied by Parakilas, but directed the Guardian to a November 2017 blogpost in which the company defended its data sharing practices, which it said had \u201csignificantly improved\u201d over the last five years.\n\n\u201cWhile it\u2019s fair to criticise how we enforced our developer policies more than five years ago, it\u2019s untrue to suggest we didn\u2019t or don\u2019t care about privacy,\u201d that statement said. \u201cThe facts tell a different story.\u201d\n\n\u2018A majority of Facebook users\u2019\n\nParakilas, 38, who now works as a product manager for Uber, is particularly critical of Facebook\u2019s previous policy of allowing developers to access the personal data of friends of people who used apps on the platform, without the knowledge or express consent of those friends.\n\nThat feature, called friends permission, was a boon to outside software developers who, from 2007 onwards, were given permission by Facebook to build quizzes and games \u2013 like the widely popular FarmVille \u2013 that were hosted on the platform.\n\nThe apps proliferated on Facebook in the years leading up to the company\u2019s 2012 initial public offering, an era when most users were still accessing the platform via laptops and computers rather than smartphones.\n\nFacebook took a 30% cut of payments made through apps, but in return enabled their creators to have access to Facebook user data.\n\nParakilas does not know how many companies sought friends permission data before such access was terminated around mid-2014. However, he said he believes tens or maybe even hundreds of thousands of developers may have done so.\n\nIt has been painful watching, because I know they could have prevented it\n\nParakilas estimates that \u201ca majority of Facebook users\u201d could have had their data harvested by app developers without their knowledge. The company now has stricter protocols around the degree of access third parties have to data.\n\nParakilas said that when he worked at Facebook it failed to take full advantage of its enforcement mechanisms, such as a clause that enables the social media giant to audit external developers who misuse its data.\n\nLegal action against rogue developers or moves to ban them from Facebook were \u201cextremely rare\u201d, he said, adding: \u201cIn the time I was there, I didn\u2019t see them conduct a single audit of a developer\u2019s systems.\u201d\n\nFacebook announced on Monday that it had hired a digital forensics firm to conduct an audit of Cambridge Analytica. The decision comes more than two years after Facebook was made aware of the reported data breach.\n\n\n\nDuring the time he was at Facebook, Parakilas said the company was keen to encourage more developers to build apps for its platform and \u201cone of the main ways to get developers interested in building apps was through offering them access to this data\u201d. Shortly after arriving at the company\u2019s Silicon Valley headquarters he was told that any decision to ban an app required the personal approval of the chief executive, Mark Zuckerberg, although the policy was later relaxed to make it easier to deal with rogue developers.\n\nWhile the previous policy of giving developers access to Facebook users\u2019 friends\u2019 data was sanctioned in the small print in Facebook\u2019s terms and conditions, and users could block such data sharing by changing their settings, Parakilas said he believed the policy was problematic.\n\n\u201cIt was well understood in the company that that presented a risk,\u201d he said. \u201cFacebook was giving data of people who had not authorised the app themselves, and was relying on terms of service and settings that people didn\u2019t read or understand.\u201d\n\nIt was this feature that was exploited by Global Science Research, and the data provided to Cambridge Analytica in 2014. GSR was run by the Cambridge University psychologist Aleksandr Kogan, who built an app that was a personality test for Facebook users.\n\nThe test automatically downloaded the data of friends of people who took the quiz, ostensibly for academic purposes. Cambridge Analytica has denied knowing the data was obtained improperly, and Kogan maintains he did nothing illegal and had a \u201cclose working relationship\u201d with Facebook.\n\nWhile Kogan\u2019s app only attracted around 270,000 users (most of whom were paid to take the quiz), the company was then able to exploit the friends permission feature to quickly amass data pertaining to more than 50 million Facebook users.\n\n\u201cKogan\u2019s app was one of the very last to have access to friend permissions,\u201d Parakilas said, adding that many other similar apps had been harvesting similar quantities of data for years for commercial purposes. Academic research from 2010, based on an analysis of 1,800 Facebooks apps, concluded that around 11% of third-party developers requested data belonging to friends of users.\n\nIf those figures were extrapolated, tens of thousands of apps, if not more, were likely to have systematically culled \u201cprivate and personally identifiable\u201d data belonging to hundreds of millions of users, Parakilas said.\n\nThe ease with which it was possible for anyone with relatively basic coding skills to create apps and start trawling for data was a particular concern, he added.\n\nParakilas said he was unsure why Facebook stopped allowing developers to access friends data around mid-2014, roughly two years after he left the company. However, he said he believed one reason may have been that Facebook executives were becoming aware that some of the largest apps were acquiring enormous troves of valuable data.\n\nHe recalled conversations with executives who were nervous about the commercial value of data being passed to other companies.\n\n\u201cThey were worried that the large app developers were building their own social graphs, meaning they could see all the connections between these people,\u201d he said. \u201cThey were worried that they were going to build their own social networks.\u201d\n\n\u2018They treated it like a PR exercise\u2019\n\nParakilas said he lobbied internally at Facebook for \u201ca more rigorous approach\u201d to enforcing data protection, but was offered little support. His warnings included a PowerPoint presentation he said he delivered to senior executives in mid-2012 \u201cthat included a map of the vulnerabilities for user data on Facebook\u2019s platform\u201d.\n\n\u201cI included the protective measures that we had tried to put in place, where we were exposed, and the kinds of bad actors who might do malicious things with the data,\u201d he said. \u201cOn the list of bad actors I included foreign state actors and data brokers.\u201d\n\nFrustrated at the lack of action, Parakilas left Facebook in late 2012. \u201cI didn\u2019t feel that the company treated my concerns seriously. I didn\u2019t speak out publicly for years out of self-interest, to be frank.\u201d\n\nHow to protect your Facebook privacy \u2013 or delete yourself completely Read more\n\nThat changed, Parakilas said, when he heard the congressional testimony given by Facebook lawyers to Senate and House investigators in late 2017 about Russia\u2019s attempt to sway the presidential election. \u201cThey treated it like a PR exercise,\u201d he said. \u201cThey seemed to be entirely focused on limiting their liability and exposure rather than helping the country address a national security issue.\u201d\n\nIt was at that point that Parakilas decided to go public with his concerns, writing an opinion article in the New York Times that said Facebook could not be trusted to regulate itself. Since then, Parakilas has become an adviser to the Center for Humane Technology, which is run by Tristan Harris, a former Google employee turned whistleblower on the industry.",
    "source_url": "www.theguardian.com",
    "bias_text": "left",
    "ID": "CK5meTNASe5EjMJO"
}