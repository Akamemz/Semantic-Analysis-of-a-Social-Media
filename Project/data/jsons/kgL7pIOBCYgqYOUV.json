{
    "topic": "national_security",
    "source": "Christian Science Monitor",
    "bias": 1,
    "url": "http://www.csmonitor.com/Daily/2017/20170606#1046544",
    "title": "Why the internet remains an ISIS training ground",
    "date": "2017-06-06",
    "authors": "",
    "content": "Collapse 2 . Why the internet remains an ISIS training ground\nThe terrorist attack on London Bridge over the weekend has reignited a debate about tech companies \u2019 level of responsibility in preventing terrorism . Hours after Saturday \u2019 s attack , British Prime Minister Theresa May called for a regulatory crackdown on online content and criticized the tech industry for giving extremist ideology \u201c the safe space it needs to breed . \u201d\nLondon Mayor Sadiq Khan echoed that call in a statement Monday . \u201c After every terrorist attack we rightly say that the internet providers and social media companies need to act and restrict access to these poisonous materials , \u201d he said . \u201c But it has not happened ... now it simply must happen . \u201d\nBut analysts say pushing technology companies to remove extremist content may not be the straightforward solution it seems .\nThere are expected censorship concerns , but , it \u2019 s not as simple as free speech versus security . Some say removing content might not be effective in disconnecting Islamic State ( ISIS ) recruiters from potential recruits , and may even make it more challenging for intelligence agencies to monitor terrorist plots online . Others suggest focusing on online content is a distraction , and efforts should instead try to prevent those susceptible to extremist messages from seeking them out online in the first place .\nThese calls come as reports surfaced that one of the three attackers responsible for Saturday \u2019 s terrorist attack may have been radicalized by extremist sermons on YouTube .\nISIS videos and other materials have also surfaced online in the past year that highlight how to maximize damage with a vehicle and knife attack \u2013 a script that is eerily similar to the London Bridge attack that left seven dead and 48 injured .\nThe open nature of the internet has long been criticized by regulatory advocates as offering terrorists a free forum to circulate extremist content . By one count , as many as 90 percent of terrorist attacks in the past four years have had an online component to them . But those opposed to a regulatory approach cite concerns that cracking down on questionable content risks casting too broad a brush , censoring legitimate content .\nWhen it comes to extremist content , treading that line is tricky . Unlike some content , such as child pornography , holding extreme views isn \u2019 t illegal \u2013 and neither is broadcasting them in the United States . As such , it takes a value judgment to decide which content to remove .\nAn algorithm can \u2019 t pick up on the necessary nuances to find the line between over-censorship and dangerous extremist content , says Aram Sinnreich , professor of communications at American University in Washington . \u201c There are no paths that preserve anything remotely approaching an open internet , and at the same time preventing ISIS from posting recruitment videos . \u201d\nMany large tech companies have tried to compromise by employing an army of human workers to review content flagged by users as problematic . The reviewers use the tech company \u2019 s terms of use as guidance , but in the case of extremist content , it \u2019 s not always black and white .\nBut Hany Farid , senior adviser to the nonprofit Counter Extremism Project , says it is possible for an algorithm to find the sweet spot , as long as humans work with it . A computer science professor at Dartmouth College , Dr. Farid helped develop the tool now used by most internet companies to identify and remove child pornography . He has also developed a more sophisticated tool that he says can be harnessed to weed out extremist content .\nFarid says internet companies \u2019 concerns about crossing the line into censorship are unfounded .\n\u201c I \u2019 m not buying the story \u201d that it \u2019 s too difficult or there \u2019 s a slippery slope leading to more censorship , Farid says . \u201c That \u2019 s a smokescreen , saying there \u2019 s a gray area . Of course there is . But it doesn \u2019 t mean we don \u2019 t do anything . You deal with the black and white cases , and deal with the gray cases when you have to . \u201d\nTech companies have gone through \u201c an evolution of thinking \u201d recently and are now more proactively removing content on their own , says Seamus Hughes , deputy director of the Program on Extremism at George Washington University . He points to the 2013 Boston Marathon bombing as a turning point . Investigators found clues that the attackers may have learned how to make a bomb from Inspire magazine , an online , English-language publication reportedly by the organization Al Qaeda .\n\u201c It became so there was less of a level of acceptance for general propaganda to be floating out there , \u201d Mr. Hughes says .\nIn one initiative launched last year , the tech giants are teaming up to make it easier to spot terrorism-related content . Facebook , Microsoft , Twitter , and YouTube have developed channels to share information about such extremist content and accounts so that individual companies can find and take it down more quickly .\nStill , some say that removing content might not actually be an effective approach to stem radicalization and recruitment by terrorist organizations .\nOne concern is that extremist content will simply move to other platforms .\n\u201c It \u2019 s sort of a whack-a-mole kind of problem , \u201d says Eric Rosand , senior fellow in the Project on US Relations with the Islamic World at the Brookings Institution and director of The Prevention Project : Organizing Against Violent Extremism in Washington , D.C. \u201c Terrorists will find another way to reach out with propaganda \u201d if it \u2019 s removed .\nThat could mean moving onto smaller platforms with more encryption and less bandwidth to review and remove content .\nThis content could also be moved to the dark web , a section of the internet that is dense with encryption and challenging for intelligence officials to track . Sure , there \u2019 s a limited audience in the dark web , a detail which could reduce recruitment for organizations like ISIS , Hughes says , but those who do make it into the depths of the dark web are particularly dedicated .\nAnd then there \u2019 s the question of where intelligence agencies can best keep tabs on extremists , Hughes says . \u201c Is it better for these guys to be on the systems where we know we can [ collect information on ] them , we know who everyone is , but they can reach more people ? Or is it better to push them off to the margins so they \u2019 re only talking to who they already were going to talk to to begin with ? \u201d\nSome tech companies and government officials have been weighing alternative options to counteract extremist content . One idea is to harness the tools of the internet and social media to reach people in danger of being radicalized \u2013 in other words , use the same tools as ISIS in a sort of counter-messaging effort .\nGoogle \u2019 s 2015 pilot project , the \u201c Redirect Method , \u201d tried to target the audience most susceptible to online recruitment and radicalization and , when they searched for certain terms , directed them toward existing YouTube videos that counter terrorists \u2019 messages . The project used similar principles that businesses use to target ads to certain consumers .\nSimilarly , officials in the State Department \u2019 s Global Engagement Center have used paid ads on Facebook as a means of reaching out to young Muslims who may be targeted by extremist recruiters . The ads are for videos and messaging that counteract what they hear from jihadists .\nBut online content might not be as responsible for radicalizing terrorists as some politicians are implying , says Dr. Rosand of Brookings . \u201c It \u2019 s as much about the offline networks , it \u2019 s as much about the grievances that drove them to violence , or made them very susceptible to violent messages , as they become radicalized . \u201d\nHe suggests that politicians instead encourage tech companies to invest in communities by providing other alternatives to the path of terrorism . \u201c How do you give them options , other than going online , to search for meaning in their lives ? We don \u2019 t invest enough in that . \u201d",
    "content_original": "Collapse 2. Why the internet remains an ISIS training ground\n\nThe terrorist attack on London Bridge over the weekend has reignited a debate about tech companies\u2019 level of responsibility in preventing terrorism. Hours after Saturday\u2019s attack, British Prime Minister Theresa May called for a regulatory crackdown on online content and criticized the tech industry for giving extremist ideology \u201cthe safe space it needs to breed.\u201d\n\nLondon Mayor Sadiq Khan echoed that call in a statement Monday. \u201cAfter every terrorist attack we rightly say that the internet providers and social media companies need to act and restrict access to these poisonous materials,\u201d he said. \u201cBut it has not happened ... now it simply must happen.\u201d\n\nBut analysts say pushing technology companies to remove extremist content may not be the straightforward solution it seems.\n\nThere are expected censorship concerns, but, it\u2019s not as simple as free speech versus security. Some say removing content might not be effective in disconnecting Islamic State (ISIS) recruiters from potential recruits, and may even make it more challenging for intelligence agencies to monitor terrorist plots online. Others suggest focusing on online content is a distraction, and efforts should instead try to prevent those susceptible to extremist messages from seeking them out online in the first place.\n\nThese calls come as reports surfaced that one of the three attackers responsible for Saturday\u2019s terrorist attack may have been radicalized by extremist sermons on YouTube.\n\nISIS videos and other materials have also surfaced online in the past year that highlight how to maximize damage with a vehicle and knife attack \u2013 a script that is eerily similar to the London Bridge attack that left seven dead and 48 injured.\n\nThe line between stifling speech and thwarting terrorism\n\nThe open nature of the internet has long been criticized by regulatory advocates as offering terrorists a free forum to circulate extremist content. By one count, as many as 90 percent of terrorist attacks in the past four years have had an online component to them. But those opposed to a regulatory approach cite concerns that cracking down on questionable content risks casting too broad a brush, censoring legitimate content.\n\nWhen it comes to extremist content, treading that line is tricky. Unlike some content, such as child pornography, holding extreme views isn\u2019t illegal \u2013 and neither is broadcasting them in the United States. As such, it takes a value judgment to decide which content to remove.\n\nAn algorithm can\u2019t pick up on the necessary nuances to find the line between over-censorship and dangerous extremist content, says Aram Sinnreich, professor of communications at American University in Washington. \u201cThere are no paths that preserve anything remotely approaching an open internet, and at the same time preventing ISIS from posting recruitment videos.\u201d\n\nMany large tech companies have tried to compromise by employing an army of human workers to review content flagged by users as problematic. The reviewers use the tech company\u2019s terms of use as guidance, but in the case of extremist content, it\u2019s not always black and white.\n\nBut Hany Farid, senior adviser to the nonprofit Counter Extremism Project, says it is possible for an algorithm to find the sweet spot, as long as humans work with it. A computer science professor at Dartmouth College, Dr. Farid helped develop the tool now used by most internet companies to identify and remove child pornography. He has also developed a more sophisticated tool that he says can be harnessed to weed out extremist content.\n\nFarid says internet companies\u2019 concerns about crossing the line into censorship are unfounded.\n\n\u201cI\u2019m not buying the story\u201d that it\u2019s too difficult or there\u2019s a slippery slope leading to more censorship, Farid says. \u201cThat\u2019s a smokescreen, saying there\u2019s a gray area. Of course there is. But it doesn\u2019t mean we don\u2019t do anything. You deal with the black and white cases, and deal with the gray cases when you have to.\u201d\n\nTech companies have gone through \u201can evolution of thinking\u201d recently and are now more proactively removing content on their own, says Seamus Hughes, deputy director of the Program on Extremism at George Washington University. He points to the 2013 Boston Marathon bombing as a turning point. Investigators found clues that the attackers may have learned how to make a bomb from Inspire magazine, an online, English-language publication reportedly by the organization Al Qaeda.\n\n\u201cIt became so there was less of a level of acceptance for general propaganda to be floating out there,\u201d Mr. Hughes says.\n\nIn one initiative launched last year, the tech giants are teaming up to make it easier to spot terrorism-related content. Facebook, Microsoft, Twitter, and YouTube have developed channels to share information about such extremist content and accounts so that individual companies can find and take it down more quickly.\n\nWhack-a-mole concerns\n\nStill, some say that removing content might not actually be an effective approach to stem radicalization and recruitment by terrorist organizations.\n\nOne concern is that extremist content will simply move to other platforms.\n\n\u201cIt\u2019s sort of a whack-a-mole kind of problem,\u201d says Eric Rosand, senior fellow in the Project on US Relations with the Islamic World at the Brookings Institution and director of The Prevention Project: Organizing Against Violent Extremism in Washington, D.C. \u201cTerrorists will find another way to reach out with propaganda\u201d if it\u2019s removed.\n\nThat could mean moving onto smaller platforms with more encryption and less bandwidth to review and remove content.\n\nThis content could also be moved to the dark web, a section of the internet that is dense with encryption and challenging for intelligence officials to track. Sure, there\u2019s a limited audience in the dark web, a detail which could reduce recruitment for organizations like ISIS, Hughes says, but those who do make it into the depths of the dark web are particularly dedicated.\n\nAnd then there\u2019s the question of where intelligence agencies can best keep tabs on extremists, Hughes says. \u201cIs it better for these guys to be on the systems where we know we can [collect information on] them, we know who everyone is, but they can reach more people? Or is it better to push them off to the margins so they\u2019re only talking to who they already were going to talk to to begin with?\u201d\n\nCounter-messaging\n\nSome tech companies and government officials have been weighing alternative options to counteract extremist content. One idea is to harness the tools of the internet and social media to reach people in danger of being radicalized \u2013 in other words, use the same tools as ISIS in a sort of counter-messaging effort.\n\nGoogle\u2019s 2015 pilot project, the \u201cRedirect Method,\u201d tried to target the audience most susceptible to online recruitment and radicalization and, when they searched for certain terms, directed them toward existing YouTube videos that counter terrorists\u2019 messages. The project used similar principles that businesses use to target ads to certain consumers.\n\nSimilarly, officials in the State Department\u2019s Global Engagement Center have used paid ads on Facebook as a means of reaching out to young Muslims who may be targeted by extremist recruiters. The ads are for videos and messaging that counteract what they hear from jihadists.\n\nBut online content might not be as responsible for radicalizing terrorists as some politicians are implying, says Dr. Rosand of Brookings. \u201cIt\u2019s as much about the offline networks, it\u2019s as much about the grievances that drove them to violence, or made them very susceptible to violent messages, as they become radicalized.\u201d\n\nHe suggests that politicians instead encourage tech companies to invest in communities by providing other alternatives to the path of terrorism. \u201cHow do you give them options, other than going online, to search for meaning in their lives? We don\u2019t invest enough in that.\u201d",
    "source_url": "www.csmonitor.com",
    "bias_text": "center",
    "ID": "kgL7pIOBCYgqYOUV"
}