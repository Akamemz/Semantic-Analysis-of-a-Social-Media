{
    "topic": "privacy",
    "source": "Vox",
    "bias": 0,
    "url": "https://www.vox.com/future-perfect/2019/8/21/20814153/bernie-sanders-ai-facial-recognition-ban-elizabeth-warren",
    "title": "Bernie Sanders\u2019s call to ban facial recognition tech for policing, explained",
    "date": "2019-08-21",
    "authors": "Sigal Samuel",
    "content": "As the growing use of facial recognition technology draws criticism from the public , Democratic presidential candidates are starting to articulate how they \u2019 d handle the tech if they \u2019 re elected . Some candidates are staking out stronger positions than others .\nIn August , Sen. Bernie Sanders became the first presidential candidate to call for a total ban on the use of facial recognition software for policing . As part of his broader criminal justice reform plan , he also called for a moratorium on the use of algorithmic risk assessment tools that aim to predict which criminals will reoffend . Critics have called such tools racially biased .\nSen. Elizabeth Warren released her own criminal justice reform plan soon after , saying she \u2019 d create a task force to \u201c establish guardrails and appropriate privacy protections \u201d for surveillance tech , including \u201c facial recognition technology and algorithms that exacerbate underlying bias. \u201d She did not promise to institute a ban .\nJuli\u00e1n Castro \u2019 s website includes a brief mention of facial recognition , but there \u2019 s little detail . It simply says Castro wants to \u201c establish guidelines for next-generation surveillance technologies , like facial recognition technology , that accounts for disparate impact and bias in their application . \u201d\nThis week , it was Sen. Kamala Harris \u2019 s turn to roll out her criminal justice plan , which says :\nKamala would work with stakeholders , including civil rights groups , technology groups , and law enforcement , to institute regulations and protections to ensure that technology used by federal law enforcement \u2014 such as facial recognition and other surveillance \u2014 does not further racial disparities or other biases . She would also invest federal money to incentivize states and localities to do the same .\nAgain , there \u2019 s no promise of a ban here . Nor have other Democratic candidates made explicit campaign promises about facial recognition , though some , like Sen. Cory Booker , have advanced legislation pushing for algorithmic accountability .\nSome find this state of affairs disappointing , especially given that the public pushback against facial recognition is gaining momentum . The California Senate passed a bill this week placing a three-year moratorium on the use of facial recognition in police body cameras . State legislatures in New York , Michigan , and Massachusetts are also considering bills to rein in the technology . Bipartisan legislation is expected to be forthcoming in Congress .\n\u201c Facial recognition poses a unique threat to human liberty and basic rights \u2014 any candidate who wants to be taken seriously on criminal justice issues should be calling for an outright ban , or at the very least a moratorium on current use of this tech . Senator Harris \u2019 s plan says she will work with civil rights and technology organizations , but she already seems to be ignoring us , \u201d said Evan Greer , deputy director of the digital rights nonprofit Fight for the Future .\nSo far , no other candidate has carved out as tough a stance as Sanders \u2019 s . His promises about facial recognition and algorithmic risk assessment stand out because they show he \u2019 s thinking seriously about the ethical risks of AI technologies . Police officers and judges currently use both these systems to guide their decisions , despite evidence that these systems are often biased against people of color .\nSanders says he won \u2019 t allow the criminal justice system to go on using algorithmic tools for predicting recidivism until they pass an audit , because \u201c we must ensure these tools do not have any implicit biases that lead to unjust or excessive sentences. \u201d As a ProPublica investigation revealed , some of the algorithms used in courtroom sentencing do lead to unjust outcomes : A black teen may steal something and get rated high-risk for committing future crimes , for example , but a white man steals something of similar value and gets rated low-risk .\nThe plan to audit these algorithms for bias makes sense . There are also several good reasons to think Sanders \u2019 s more radical plan to completely ban facial recognition in policing is warranted . Let \u2019 s break them down .\nFacial recognition software , which can identify an individual by analyzing their facial features in images , in videos , or in real time , has encountered a growing backlash over the past few months . Behemoth companies like Apple , Amazon , and Microsoft are all mired in controversy over it . San Francisco , Oakland , and Somerville have all issued local bans .\nSome argue that outlawing facial recognition tech is throwing the proverbial baby out with the bathwater . Advocates say the software can help with worthy aims , like finding missing children and elderly adults or catching criminals and terrorists . Microsoft president Brad Smith has said it would be \u201c cruel \u201d to altogether stop selling the software to government agencies . This camp wants to see the tech regulated , not banned .\nYet there \u2019 s good reason to think regulation won \u2019 t be enough . The danger of this tech is not well understood by the general public , and the market for it is so lucrative that there are strong financial incentives to keep pushing it into more areas of our lives in the absence of a ban . AI is also developing so fast that regulators would likely have to play whack-a-mole as they struggle to keep up with evolving forms of facial recognition .\nThen there \u2019 s the well-documented fact that human bias can creep into AI . Often , this manifests as a problem with the training data that goes into AIs : If designers mostly feed the systems examples of white male faces , and don \u2019 t think to diversify their data , the systems won \u2019 t learn to properly recognize women and people of color . And indeed , we \u2019 ve found that facial recognition systems often misidentify those groups , which could lead to them being disproportionately held for questioning when law enforcement agencies put the tech to use .\nIn 2015 , Google \u2019 s image recognition system labeled African Americans as \u201c gorillas. \u201d Three years later , Amazon \u2019 s Rekognition system wrongly matched 28 members of Congress to criminal mug shots . Another study found that three facial recognition systems \u2014 IBM , Microsoft , and China \u2019 s Megvii \u2014 were more likely to misidentify the gender of dark-skinned people ( especially women ) than of light-skinned people .\nEven if all the technical issues were to be fixed and facial recognition tech completely de-biased , would that stop the software from harming our society when it \u2019 s deployed in the real world ? Not necessarily , as a recent report from the AI Now Institute explains .\nSay the tech gets just as good at identifying black people as it is at identifying white people . That may not actually be a positive change . Given that the black community is already overpoliced in the US , making black faces more legible to this tech and then giving the tech to police could just exacerbate discrimination . As Zo\u00e9 Samudzi wrote at the Daily Beast , \u201c It is not social progress to make black people equally visible to software that will inevitably be further weaponized against us . \u201d\nWoodrow Hartzog and Evan Selinger , a law professor and a philosophy professor , respectively , argued last year that facial recognition tech is inherently damaging to our social fabric . \u201c The mere existence of facial recognition systems , which are often invisible , harms civil liberties , because people will act differently if they suspect they \u2019 re being surveilled , \u201d they wrote . The worry is that there \u2019 ll be a chilling effect on freedom of speech , assembly , and religion .\nThe authors also note that our faces are something we can \u2019 t change ( at least not without surgery ) , that they \u2019 re central to our identity , and that they \u2019 re all too easily captured from a distance ( unlike fingerprints or iris scans ) . If we don \u2019 t ban facial recognition before it becomes more entrenched , they argue , \u201c people won \u2019 t know what it \u2019 s like to be in public without being automatically identified , profiled , and potentially exploited . \u201d\nLuke Stark , a digital media scholar who works for Microsoft Research Montreal , made another argument for a ban in a recent article titled \u201c Facial recognition is the plutonium of AI . \u201d\nComparing software to a radioactive element may seem over the top , but Stark insists the analogy is apt . Plutonium is the biologically toxic element used to make atomic bombs , and just as its toxicity comes from its chemical structure , the danger of facial recognition is ineradicably , structurally embedded within it , because it attaches numerical values to the human face . He explains :\nFacial recognition technologies and other systems for visually classifying human bodies through data are inevitably and always means by which \u201c race , \u201d as a constructed category , is defined and made visible . Reducing humans into sets of legible , manipulable signs has been a hallmark of racializing scientific and administrative techniques going back several hundred years .\nThe mere fact of numerically classifying and schematizing human facial features is dangerous , he says , because it enables governments and companies to divide us into different races . It \u2019 s a short leap from having that capability to \u201c finding numerical reasons for construing some groups as subordinate , and then reifying that subordination by wielding the \u2018 charisma of numbers \u2019 to claim subordination is a \u2018 natural \u2019 fact . \u201d\nIn other words , racial categorization too often feeds racial discrimination . This is not a far-off hypothetical but a current reality : China is already using facial recognition to track Uighur Muslims based on their appearance , in a system the New York Times has dubbed \u201c automated racism. \u201d That system makes it easier for China to round up Uighurs and detain them in internment camps .\nA ban is an extreme measure , yes . But a tool that enables a government to immediately identify us anytime we cross the street is so inherently dangerous that treating it with extreme caution makes sense .\nInstead of starting from the assumption that facial recognition is permissible \u2014 which is the de facto reality we \u2019 ve unwittingly gotten used to as tech companies marketed the software to us unencumbered by legislation \u2014 we \u2019 d do better to start from the assumption that it \u2019 s banned , then carve out rare exceptions for specific cases when it might be warranted .",
    "content_original": "As the growing use of facial recognition technology draws criticism from the public, Democratic presidential candidates are starting to articulate how they\u2019d handle the tech if they\u2019re elected. Some candidates are staking out stronger positions than others.\n\nIn August, Sen. Bernie Sanders became the first presidential candidate to call for a total ban on the use of facial recognition software for policing. As part of his broader criminal justice reform plan, he also called for a moratorium on the use of algorithmic risk assessment tools that aim to predict which criminals will reoffend. Critics have called such tools racially biased.\n\nSen. Elizabeth Warren released her own criminal justice reform plan soon after, saying she\u2019d create a task force to \u201cestablish guardrails and appropriate privacy protections\u201d for surveillance tech, including \u201cfacial recognition technology and algorithms that exacerbate underlying bias.\u201d She did not promise to institute a ban.\n\nJuli\u00e1n Castro\u2019s website includes a brief mention of facial recognition, but there\u2019s little detail. It simply says Castro wants to \u201cestablish guidelines for next-generation surveillance technologies, like facial recognition technology, that accounts for disparate impact and bias in their application.\u201d\n\nThis week, it was Sen. Kamala Harris\u2019s turn to roll out her criminal justice plan, which says:\n\nKamala would work with stakeholders, including civil rights groups, technology groups, and law enforcement, to institute regulations and protections to ensure that technology used by federal law enforcement \u2014 such as facial recognition and other surveillance \u2014 does not further racial disparities or other biases. She would also invest federal money to incentivize states and localities to do the same.\n\nAgain, there\u2019s no promise of a ban here. Nor have other Democratic candidates made explicit campaign promises about facial recognition, though some, like Sen. Cory Booker, have advanced legislation pushing for algorithmic accountability.\n\nSome find this state of affairs disappointing, especially given that the public pushback against facial recognition is gaining momentum. The California Senate passed a bill this week placing a three-year moratorium on the use of facial recognition in police body cameras. State legislatures in New York, Michigan, and Massachusetts are also considering bills to rein in the technology. Bipartisan legislation is expected to be forthcoming in Congress.\n\n\u201cFacial recognition poses a unique threat to human liberty and basic rights \u2014 any candidate who wants to be taken seriously on criminal justice issues should be calling for an outright ban, or at the very least a moratorium on current use of this tech. Senator Harris\u2019s plan says she will work with civil rights and technology organizations, but she already seems to be ignoring us,\u201d said Evan Greer, deputy director of the digital rights nonprofit Fight for the Future.\n\nSo far, no other candidate has carved out as tough a stance as Sanders\u2019s. His promises about facial recognition and algorithmic risk assessment stand out because they show he\u2019s thinking seriously about the ethical risks of AI technologies. Police officers and judges currently use both these systems to guide their decisions, despite evidence that these systems are often biased against people of color.\n\nSanders says he won\u2019t allow the criminal justice system to go on using algorithmic tools for predicting recidivism until they pass an audit, because \u201cwe must ensure these tools do not have any implicit biases that lead to unjust or excessive sentences.\u201d As a ProPublica investigation revealed, some of the algorithms used in courtroom sentencing do lead to unjust outcomes: A black teen may steal something and get rated high-risk for committing future crimes, for example, but a white man steals something of similar value and gets rated low-risk.\n\nThe plan to audit these algorithms for bias makes sense. There are also several good reasons to think Sanders\u2019s more radical plan to completely ban facial recognition in policing is warranted. Let\u2019s break them down.\n\nThe case for banning facial recognition tech\n\nFacial recognition software, which can identify an individual by analyzing their facial features in images, in videos, or in real time, has encountered a growing backlash over the past few months. Behemoth companies like Apple, Amazon, and Microsoft are all mired in controversy over it. San Francisco, Oakland, and Somerville have all issued local bans.\n\nSome argue that outlawing facial recognition tech is throwing the proverbial baby out with the bathwater. Advocates say the software can help with worthy aims, like finding missing children and elderly adults or catching criminals and terrorists. Microsoft president Brad Smith has said it would be \u201ccruel\u201d to altogether stop selling the software to government agencies. This camp wants to see the tech regulated, not banned.\n\nYet there\u2019s good reason to think regulation won\u2019t be enough. The danger of this tech is not well understood by the general public, and the market for it is so lucrative that there are strong financial incentives to keep pushing it into more areas of our lives in the absence of a ban. AI is also developing so fast that regulators would likely have to play whack-a-mole as they struggle to keep up with evolving forms of facial recognition.\n\nThen there\u2019s the well-documented fact that human bias can creep into AI. Often, this manifests as a problem with the training data that goes into AIs: If designers mostly feed the systems examples of white male faces, and don\u2019t think to diversify their data, the systems won\u2019t learn to properly recognize women and people of color. And indeed, we\u2019ve found that facial recognition systems often misidentify those groups, which could lead to them being disproportionately held for questioning when law enforcement agencies put the tech to use.\n\nIn 2015, Google\u2019s image recognition system labeled African Americans as \u201cgorillas.\u201d Three years later, Amazon\u2019s Rekognition system wrongly matched 28 members of Congress to criminal mug shots. Another study found that three facial recognition systems \u2014 IBM, Microsoft, and China\u2019s Megvii \u2014 were more likely to misidentify the gender of dark-skinned people (especially women) than of light-skinned people.\n\nEven if all the technical issues were to be fixed and facial recognition tech completely de-biased, would that stop the software from harming our society when it\u2019s deployed in the real world? Not necessarily, as a recent report from the AI Now Institute explains.\n\nSay the tech gets just as good at identifying black people as it is at identifying white people. That may not actually be a positive change. Given that the black community is already overpoliced in the US, making black faces more legible to this tech and then giving the tech to police could just exacerbate discrimination. As Zo\u00e9 Samudzi wrote at the Daily Beast, \u201cIt is not social progress to make black people equally visible to software that will inevitably be further weaponized against us.\u201d\n\nWoodrow Hartzog and Evan Selinger, a law professor and a philosophy professor, respectively, argued last year that facial recognition tech is inherently damaging to our social fabric. \u201cThe mere existence of facial recognition systems, which are often invisible, harms civil liberties, because people will act differently if they suspect they\u2019re being surveilled,\u201d they wrote. The worry is that there\u2019ll be a chilling effect on freedom of speech, assembly, and religion.\n\nThe authors also note that our faces are something we can\u2019t change (at least not without surgery), that they\u2019re central to our identity, and that they\u2019re all too easily captured from a distance (unlike fingerprints or iris scans). If we don\u2019t ban facial recognition before it becomes more entrenched, they argue, \u201cpeople won\u2019t know what it\u2019s like to be in public without being automatically identified, profiled, and potentially exploited.\u201d\n\nLuke Stark, a digital media scholar who works for Microsoft Research Montreal, made another argument for a ban in a recent article titled \u201cFacial recognition is the plutonium of AI.\u201d\n\nComparing software to a radioactive element may seem over the top, but Stark insists the analogy is apt. Plutonium is the biologically toxic element used to make atomic bombs, and just as its toxicity comes from its chemical structure, the danger of facial recognition is ineradicably, structurally embedded within it, because it attaches numerical values to the human face. He explains:\n\nFacial recognition technologies and other systems for visually classifying human bodies through data are inevitably and always means by which \u201crace,\u201d as a constructed category, is defined and made visible. Reducing humans into sets of legible, manipulable signs has been a hallmark of racializing scientific and administrative techniques going back several hundred years.\n\nThe mere fact of numerically classifying and schematizing human facial features is dangerous, he says, because it enables governments and companies to divide us into different races. It\u2019s a short leap from having that capability to \u201cfinding numerical reasons for construing some groups as subordinate, and then reifying that subordination by wielding the \u2018charisma of numbers\u2019 to claim subordination is a \u2018natural\u2019 fact.\u201d\n\nIn other words, racial categorization too often feeds racial discrimination. This is not a far-off hypothetical but a current reality: China is already using facial recognition to track Uighur Muslims based on their appearance, in a system the New York Times has dubbed \u201cautomated racism.\u201d That system makes it easier for China to round up Uighurs and detain them in internment camps.\n\nA ban is an extreme measure, yes. But a tool that enables a government to immediately identify us anytime we cross the street is so inherently dangerous that treating it with extreme caution makes sense.\n\nInstead of starting from the assumption that facial recognition is permissible \u2014 which is the de facto reality we\u2019ve unwittingly gotten used to as tech companies marketed the software to us unencumbered by legislation \u2014 we\u2019d do better to start from the assumption that it\u2019s banned, then carve out rare exceptions for specific cases when it might be warranted.\n\nSign up for the Future Perfect newsletter. Twice a week, you\u2019ll get a roundup of ideas and solutions for tackling our biggest challenges: improving public health, decreasing human and animal suffering, easing catastrophic risks, and \u2014 to put it simply \u2014 getting better at doing good.",
    "source_url": "www.vox.com",
    "bias_text": "left",
    "ID": "JekOU3JPi77mBbbp"
}