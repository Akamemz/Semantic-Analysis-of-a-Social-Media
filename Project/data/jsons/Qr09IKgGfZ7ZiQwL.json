{
    "topic": "polarization",
    "source": "New York Times - News",
    "bias": 0,
    "url": "http://www.nytimes.com/2015/05/08/technology/facebook-study-disputes-theory-of-political-polarization-among-users.html?_r=0",
    "title": "Facebook Study Disputes Theory of Political Polarization Among Users",
    "date": "2015-05-08",
    "authors": "Farhad Manjoo",
    "content": "The findings are convenient for Facebook . With more than 1.3 billion users , the social network is effectively the world \u2019 s most widely read daily newspaper . About 30 percent of American adults get their news from Facebook , according to the Pew Research Center . But its editorial decisions are drafted with little transparency using the News Feed algorithm . Facebook could use the study \u2019 s results to show that the algorithm is not ruining national discourse .\nFacebook said its researchers had wide latitude to pursue their research interests and to present whatever they found . The results were reviewed before publication in Science , with the journal selecting an anonymous panel of scholars unaffiliated with Facebook . Science does not disclose the identity of experts and warns reviewers to declare any financial ties that might be perceived as a conflict of interest with the study being reviewed .\nFacebook also noted that this study was substantively different from one that caused an outcry last year , in which the company \u2019 s scientists altered the number of positive and negative posts that some people saw to examine the effects on mood . This study did not involve an experiment that changed users \u2019 experience of Facebook ; researchers analyzed how people use Facebook as it stands today .\nFor Facebook \u2019 s study , researchers first determined the point of view of a given article by looking at whether liberals or conservatives had shared it most . They found unsurprising partisan attitudes about some news sources , with Fox News stories shared mainly by conservatives and Huffington Post articles shared by liberals .\nThen they measured how often feeds of users , whose identifying details had been taken out , displayed stories that conflicted with their professed ideologies , and how often they clicked on those stories .\nSome academics said Facebook was always tweaking the News Feed and could easily make changes that would create a more sealed echo chamber .\n\u201c A small effect today might become a large effect tomorrow , \u201d David Lazer , a political scientist at Northeastern University who studies social networks , wrote in a commentary on the Facebook study also published in Science . \u201c The deliberative sky is not yet falling , but the skies are not completely clear either . \u201d",
    "content_original": "The findings are convenient for Facebook. With more than 1.3 billion users, the social network is effectively the world\u2019s most widely read daily newspaper. About 30 percent of American adults get their news from Facebook, according to the Pew Research Center. But its editorial decisions are drafted with little transparency using the News Feed algorithm. Facebook could use the study\u2019s results to show that the algorithm is not ruining national discourse.\n\nFacebook said its researchers had wide latitude to pursue their research interests and to present whatever they found. The results were reviewed before publication in Science, with the journal selecting an anonymous panel of scholars unaffiliated with Facebook. Science does not disclose the identity of experts and warns reviewers to declare any financial ties that might be perceived as a conflict of interest with the study being reviewed.\n\nFacebook also noted that this study was substantively different from one that caused an outcry last year, in which the company\u2019s scientists altered the number of positive and negative posts that some people saw to examine the effects on mood. This study did not involve an experiment that changed users\u2019 experience of Facebook; researchers analyzed how people use Facebook as it stands today.\n\nFor Facebook\u2019s study, researchers first determined the point of view of a given article by looking at whether liberals or conservatives had shared it most. They found unsurprising partisan attitudes about some news sources, with Fox News stories shared mainly by conservatives and Huffington Post articles shared by liberals.\n\nThen they measured how often feeds of users, whose identifying details had been taken out, displayed stories that conflicted with their professed ideologies, and how often they clicked on those stories.\n\nSome academics said Facebook was always tweaking the News Feed and could easily make changes that would create a more sealed echo chamber.\n\n\u201cA small effect today might become a large effect tomorrow,\u201d David Lazer, a political scientist at Northeastern University who studies social networks, wrote in a commentary on the Facebook study also published in Science. \u201cThe deliberative sky is not yet falling, but the skies are not completely clear either.\u201d",
    "source_url": "www.nytimes.com",
    "bias_text": "left",
    "ID": "Qr09IKgGfZ7ZiQwL"
}