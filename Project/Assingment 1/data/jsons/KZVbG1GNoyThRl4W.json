{
    "topic": "facts_and_fact_checking",
    "source": "Vice",
    "bias": 0,
    "url": "https://www.vice.com/en_us/article/epgqxa/its-official-facebooks-fact-checking-is-making-its-fake-news-problem-even-worse",
    "title": "It\u2019s Official: Facebook\u2019s Fact-Checking Is Making Its Fake News Problem Even Worse",
    "date": "2020-03-05",
    "authors": "",
    "content": "Want the best of \u2588\u2588\u2588 News straight to your inbox ? Sign up here .\nFour years after Facebook decided it needed to do something something to fix its fake news problems , we now know those efforts only made things worse .\nAs the world was still coming to terms with President Donald Trump \u2019 s victory in the 2016 election , Facebook rolled out a program that December for independent fact-checkers to flag questionable content as \u201c disputed. \u201d But a new study out of MIT found that people assume that if some articles have warnings , those that don \u2019 t must be accurate .\n\u201c Putting a warning on some content is going to make you think , to some extent , that all of the other content without the warning might have been checked and verified , \u201d David Rand , one of the authors of the report and a professor at the MIT Sloan School of Management , said in a statement .\nRand and his co-authors have called it the \u201c implied truth effect \u201d and because of the sheer scale of Facebook \u2019 s platform \u2014 which has 1.25 billion daily users \u2014 and the amount of content posted on it , fact-checkers simply can \u2019 t keep up .\n\u201c There \u2019 s no way the fact-checkers can keep up with the stream of misinformation , so even if the warnings do really reduce belief in the tagged stories , you still have a problem , because of the implied truth effect , \u201d Rand adds .\nFacebook did not respond to questions about the study \u2019 s findings .\nThe MIT team conducted studies with more than 6,000 participants , who were shown a variety of real and fake news headlines as they would look on Facebook .\nREAD : Mark Zuckerberg is literally begging Europe to regulate Facebook : \u2018 It will be better for everyone \u2019\nOne half of the group was shown stories without fact-checking tags of any sort . The other half was shown a typical Facebook feed comprising a mixture of marked and unmarked posts .\nThen people were asked if they believed the headlines were accurate .\nThe results found that the use of \u201c false \u201d tags that are used to flag inaccurate content did significantly reduce participants \u2019 willingness to share fake stories from 29.2 % to 16.1 % . But , the study also found that unmarked false stories were believed to be true and shared 36.2 % of the time .\nOver one-fifth of the study \u2019 s participants said they believed that the unmarked posts had already been fact-checked .\nRand , an unpaid advisor to Facebook \u2019 s efforts to combat fake news , says that one obvious fix is to simply employ more fact-checkers so that all news content posted to Facebook is checked . And , he says , ordinary Facebook users could be recruited to do this work .\nREAD : So Facebook cleared things up for 2020 : politicians can totally lie to users\nWhile many people assume that allowing Facebook users to do this work would make the situation much worse , given the partisan nature of content already on the site , Rand \u2019 s research has found the opposite .\nIn fact , this is something Facebook is already considering . In December it announced it would be hiring some part-time \u201c community reviewers \u201d who will help corroborate or debunk content flagged by the company \u2019 s automated systems .\nFacebook wouldn \u2019 t say at the time how many reviewers it would hire or how much content they would be reviewing .\n\u201c If , in addition to putting warnings on things fact-checkers find to be false , you also put verification panels on things fact-checkers find to be true , then that solves the problem because there \u2019 s no longer any ambiguity , \u201d Rand said . \u201c If you see a story without a label , you know it simply hasn \u2019 t been checked . \u201d\nCover : The Facebook logo on the display of a smartphone . Photo by : Soeren Stache/picture-alliance/dpa/AP Images",
    "content_original": "Want the best of VICE News straight to your inbox? Sign up here.\n\nFour years after Facebook decided it needed to do something something to fix its fake news problems, we now know those efforts only made things worse.\n\nAs the world was still coming to terms with President Donald Trump\u2019s victory in the 2016 election, Facebook rolled out a program that December for independent fact-checkers to flag questionable content as \u201cdisputed.\u201d But a new study out of MIT found that people assume that if some articles have warnings, those that don\u2019t must be accurate.\n\n\u201cPutting a warning on some content is going to make you think, to some extent, that all of the other content without the warning might have been checked and verified,\u201d David Rand, one of the authors of the report and a professor at the MIT Sloan School of Management, said in a statement.\n\nRand and his co-authors have called it the \u201cimplied truth effect\u201d and because of the sheer scale of Facebook\u2019s platform \u2014 which has 1.25 billion daily users \u2014 and the amount of content posted on it, fact-checkers simply can\u2019t keep up.\n\n\u201cThere\u2019s no way the fact-checkers can keep up with the stream of misinformation, so even if the warnings do really reduce belief in the tagged stories, you still have a problem, because of the implied truth effect,\u201d Rand adds.\n\nFacebook did not respond to questions about the study\u2019s findings.\n\nThe MIT team conducted studies with more than 6,000 participants, who were shown a variety of real and fake news headlines as they would look on Facebook.\n\nREAD: Mark Zuckerberg is literally begging Europe to regulate Facebook: \u2018It will be better for everyone\u2019\n\nOne half of the group was shown stories without fact-checking tags of any sort. The other half was shown a typical Facebook feed comprising a mixture of marked and unmarked posts.\n\nThen people were asked if they believed the headlines were accurate.\n\nThe results found that the use of \u201cfalse\u201d tags that are used to flag inaccurate content did significantly reduce participants\u2019 willingness to share fake stories from 29.2% to 16.1%. But, the study also found that unmarked false stories were believed to be true and shared 36.2% of the time.\n\nOver one-fifth of the study\u2019s participants said they believed that the unmarked posts had already been fact-checked.\n\nRand, an unpaid advisor to Facebook\u2019s efforts to combat fake news, says that one obvious fix is to simply employ more fact-checkers so that all news content posted to Facebook is checked. And, he says, ordinary Facebook users could be recruited to do this work.\n\nREAD: So Facebook cleared things up for 2020: politicians can totally lie to users\n\nWhile many people assume that allowing Facebook users to do this work would make the situation much worse, given the partisan nature of content already on the site, Rand\u2019s research has found the opposite.\n\nIn fact, this is something Facebook is already considering. In December it announced it would be hiring some part-time \u201ccommunity reviewers\u201d who will help corroborate or debunk content flagged by the company\u2019s automated systems.\n\nFacebook wouldn\u2019t say at the time how many reviewers it would hire or how much content they would be reviewing.\n\nAnother solution is simply applying more labels.\n\n\u201cIf, in addition to putting warnings on things fact-checkers find to be false, you also put verification panels on things fact-checkers find to be true, then that solves the problem because there\u2019s no longer any ambiguity,\u201d Rand said. \u201cIf you see a story without a label, you know it simply hasn\u2019t been checked.\u201d\n\nCover: The Facebook logo on the display of a smartphone. Photo by: Soeren Stache/picture-alliance/dpa/AP Images",
    "source_url": "www.vice.com",
    "bias_text": "left",
    "ID": "KZVbG1GNoyThRl4W"
}