{
    "topic": "polarization",
    "source": "New York Magazine",
    "bias": 0,
    "url": "http://nymag.com/scienceofus/2015/05/problems-with-facebooks-polarization-study.html",
    "title": "The Problems With Facebook\u2019s Polarization Study",
    "date": "2015-05-08",
    "authors": "",
    "content": "Yesterday , Facebook released a study in Science that pushed back on the idea of the \u201c filter bubble \u201c : that social media creates a kind of echo chamber in which users never see arguments from the other side , helping to insulate those users from substantive political debate . Taken to its extreme , the Filter Bubble might almost completely close users off to new ideas and information , leaving them in a digital world where their viewpoints go ever unchallenged \u2014 and contributing to political polarization .\n\u201c Facebook Use Polarizing ? Site Begs to Differ , \u201d the New York Times headline read . \u201c You would think that if there was an echo chamber , you would not be exposed to any conflicting information , \u201d a data scientist who worked on the study said , \u201c but that \u2019 s not the case here . \u201d\nBut looking at the study , I came to very different conclusions . It absolutely shows that the \u201c filter bubble \u201d exists among some users , and that Facebook and its algorithms play a significant role in creating that bubble . But I can only make that claim about a small number of users that are likely not at all representative of the broader Facebook population , because Facebook relied on such an unusual sample of its users . In other words , despite the buzz this study is getting , we still don \u2019 t have a very good sense of how Facebook and other social-media services might or might not contribute to polarization .\nAs pointed out by Nathan Jurgenson , the study only looks at people who self-identify their political orientation on the site . That means it only examines 9 percent of users , a number you \u2019 d only see if you read through the appendix , he notes . It also only looks at Facebook users who log in four to seven days a week , and who meet a few other criteria as well . That nudges the proportion of users examined in the study down to just 4 percent , or about 10 million users .\nAre those 4 percent of users representative of Facebook \u2019 s user base as a whole ? Well , they aren \u2019 t randomly sampled . We know that identifying your political affiliation is a fairly rare behavior , given that fewer than one in ten users bothers to do so . My guess is that those users are much more politically engaged and likely much more partisan than the average user , and that \u2019 s probably going to affect what they click on and whom they are friends with . But how and how much that matters , I can not say .\nThere could also be something different about those Facebook users who log in frequently \u2014 maybe they post more or leave more comments . Again , we don \u2019 t know . But the point is that if you want to make any broad conclusions about a big population based on a study , you need a random , representative sample . You can \u2019 t survey three rich guys in Greenwich and declare that \u201c America \u2019 s \u201d favorite food is caviar .\nSocial-media experts and data scientists are taking Facebook to task for not making all this clearer . \u201c At first , I read this quickly and I took this to mean that out of the at least 200 million Americans who have used Facebook , the researchers selected a \u2018 large \u2019 sample that was representative of Facebook users , \u201d writes Christian Sandvig of the University of Michigan . \u201c The \u2018 limitations \u2019 section discusses the demographics of \u2018 Facebook \u2019 s users , \u2019 as would be the normal thing to do if they were sampled . There is no information about the selection procedure in the article itself . \u201d\nBut even setting aside the sample issues , the study clearly does not show that those unusual users are exposed to a diverse set of viewpoints , nudged along by the Facebook algorithm . It shows that they see a fairly skewed set of viewpoints , with the Facebook algorithm contributing to the skew . Facebook filters out about 1 in 20 \u201c cross-cutting \u201d hard-news stories for conservatives and about 1 in 13 \u201c cross-cutting \u201d hard-news stories for liberals .\nFacebook attempts to distance itself from filter-bubble accusations by noting that individuals isolate themselves , too , with self-identified conservatives clicking on 17 percent fewer \u201c cross-cutting \u201d news stories than would be expected if they clicked at random , and liberals , 6 percent fewer . The company used that finding to argue that it plays less of a role than individuals themselves . But that \u2019 s only true for conservatives \u2014 not for liberals .\nAnd it \u2019 s not clear that Facebook can or should be arguing that it plays a smaller filtering role than individuals , given how the study was conducted in the first place and given that the two findings do not seem directly comparable . \u201c I can not remember a worse apples to oranges comparison I \u2019 ve seen recently , especially since these two dynamics , algorithmic suppression and individual choice , have cumulative effects , \u201d writes Zeynep Tufekci of the University of North Carolina .\nNo , the filter bubble feels like a very real phenomenon , and Facebook has just shown that for some users , it contributes to it . On social media , we hear what we want to hear , see what we want to see , and click what we want to click . Don \u2019 t let Facebook tell you otherwise .",
    "content_original": "Yesterday, Facebook released a study in Science that pushed back on the idea of the \u201cfilter bubble\u201c: that social media creates a kind of echo chamber in which users never see arguments from the other side, helping to insulate those users from substantive political debate. Taken to its extreme, the Filter Bubble might almost completely close users off to new ideas and information, leaving them in a digital world where their viewpoints go ever unchallenged \u2014 and contributing to political polarization.\n\n\u201cFacebook Use Polarizing? Site Begs to Differ,\u201d the New York Times headline read. \u201cYou would think that if there was an echo chamber, you would not be exposed to any conflicting information,\u201d a data scientist who worked on the study said, \u201cbut that\u2019s not the case here.\u201d\n\nBut looking at the study, I came to very different conclusions. It absolutely shows that the \u201cfilter bubble\u201d exists among some users, and that Facebook and its algorithms play a significant role in creating that bubble. But I can only make that claim about a small number of users that are likely not at all representative of the broader Facebook population, because Facebook relied on such an unusual sample of its users. In other words, despite the buzz this study is getting, we still don\u2019t have a very good sense of how Facebook and other social-media services might or might not contribute to polarization.\n\nAs pointed out by Nathan Jurgenson, the study only looks at people who self-identify their political orientation on the site. That means it only examines 9 percent of users, a number you\u2019d only see if you read through the appendix, he notes. It also only looks at Facebook users who log in four to seven days a week, and who meet a few other criteria as well. That nudges the proportion of users examined in the study down to just 4 percent, or about 10 million users.\n\nAre those 4 percent of users representative of Facebook\u2019s user base as a whole? Well, they aren\u2019t randomly sampled. We know that identifying your political affiliation is a fairly rare behavior, given that fewer than one in ten users bothers to do so. My guess is that those users are much more politically engaged and likely much more partisan than the average user, and that\u2019s probably going to affect what they click on and whom they are friends with. But how and how much that matters, I cannot say.\n\nThere could also be something different about those Facebook users who log in frequently \u2014 maybe they post more or leave more comments. Again, we don\u2019t know. But the point is that if you want to make any broad conclusions about a big population based on a study, you need a random, representative sample. You can\u2019t survey three rich guys in Greenwich and declare that \u201cAmerica\u2019s\u201d favorite food is caviar.\n\nSocial-media experts and data scientists are taking Facebook to task for not making all this clearer. \u201cAt first, I read this quickly and I took this to mean that out of the at least 200 million Americans who have used Facebook, the researchers selected a \u2018large\u2019 sample that was representative of Facebook users,\u201d writes Christian Sandvig of the University of Michigan. \u201cThe \u2018limitations\u2019 section discusses the demographics of \u2018Facebook\u2019s users,\u2019 as would be the normal thing to do if they were sampled. There is no information about the selection procedure in the article itself.\u201d\n\nBut even setting aside the sample issues, the study clearly does not show that those unusual users are exposed to a diverse set of viewpoints, nudged along by the Facebook algorithm. It shows that they see a fairly skewed set of viewpoints, with the Facebook algorithm contributing to the skew. Facebook filters out about 1 in 20 \u201ccross-cutting\u201d hard-news stories for conservatives and about 1 in 13 \u201ccross-cutting\u201d hard-news stories for liberals.\n\nFacebook attempts to distance itself from filter-bubble accusations by noting that individuals isolate themselves, too, with self-identified conservatives clicking on 17 percent fewer \u201ccross-cutting\u201d news stories than would be expected if they clicked at random, and liberals, 6 percent fewer. The company used that finding to argue that it plays less of a role than individuals themselves. But that\u2019s only true for conservatives \u2014 not for liberals.\n\nAnd it\u2019s not clear that Facebook can or should be arguing that it plays a smaller filtering role than individuals, given how the study was conducted in the first place and given that the two findings do not seem directly comparable. \u201cI cannot remember a worse apples to oranges comparison I\u2019ve seen recently, especially since these two dynamics, algorithmic suppression and individual choice, have cumulative effects,\u201d writes Zeynep Tufekci of the University of North Carolina.\n\nNo, the filter bubble feels like a very real phenomenon, and Facebook has just shown that for some users, it contributes to it. On social media, we hear what we want to hear, see what we want to see, and click what we want to click. Don\u2019t let Facebook tell you otherwise.",
    "source_url": "www.nymag.com",
    "bias_text": "left",
    "ID": "G0dGlIMVuBMo6haR"
}