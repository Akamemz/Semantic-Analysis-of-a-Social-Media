{
    "topic": "technology",
    "source": "Associated Press",
    "bias": 1,
    "url": "https://apnews.com/ca5e62255bb87bf1b151f9bf075aaadf",
    "title": "3 crashes, 3 deaths raise questions about Tesla\u2019s Autopilot",
    "date": "2020-01-03",
    "authors": "Tom Krisher",
    "content": "FILE - In this July 8 , 2018 , file photo , clouds are reflected above the company logo on the hood of a Tesla vehicle outside a showroom in Littleton , Colo . The National Highway Traffic Safety Administration is investigating the crash of a speeding Tesla that killed two people in a Los Angeles suburb , the agency announced Tuesday , Dec. 31 , 2019 . ( AP Photo/David Zalubowski , File )\nFILE - In this July 8 , 2018 , file photo , clouds are reflected above the company logo on the hood of a Tesla vehicle outside a showroom in Littleton , Colo . The National Highway Traffic Safety Administration is investigating the crash of a speeding Tesla that killed two people in a Los Angeles suburb , the agency announced Tuesday , Dec. 31 , 2019 . ( AP Photo/David Zalubowski , File )\nDETROIT ( AP ) \u2014 Three crashes involving Teslas that killed three people have increased scrutiny of the company \u2019 s Autopilot driving system just months before CEO Elon Musk has planned to put fully self-driving cars on the streets .\nOn Sunday , a Tesla Model S sedan left a freeway in Gardena , California , at a high speed , ran a red light and struck a Honda Civic , killing two people inside , police said .\nOn the same day , a Tesla Model 3 hit a parked firetruck on an Indiana freeway , killing a passenger in the Tesla .\nAnd on Dec. 7 , yet another Model 3 struck a police cruiser on a Connecticut highway , though no one was hurt .\nThe special crash investigation unit of the National Highway Traffic Safety Administration is looking into the California crash . The agency hasn \u2019 t decided whether its special-crash unit will review the crash that occurred Sunday near Terre Haute , Indiana . In both cases , authorities have yet to determine whether Tesla \u2019 s Autopilot system was being used .\nNHTSA also is investigating the Connecticut crash , in which the driver told police that the car was operating on Autopilot , a Tesla system designed to keep a car in its lane and a safe distance from other vehicles . Autopilot also can change lanes on its own .\nTesla has said repeatedly that its Autopilot system is designed only to assist drivers , who must still pay attention and be ready to intervene at all times . The company contends that Teslas with Autopilot are safer than vehicles without it , but cautions that the system does not prevent all crashes .\nEven so , experts and safety advocates say a string of Tesla crashes raises serious questions about whether drivers have become too reliant on Tesla \u2019 s technology and whether the company does enough to ensure that drivers keep paying attention . Some critics have said it \u2019 s past time for NHTSA to stop investigating and to take action , such as forcing Tesla to make sure drivers pay attention when the system is being used .\nNHTSA has started investigations into 13 Tesla crashes dating to at least 2016 in which the agency believes Autopilot was operating . The agency has yet to issue any regulations , though it is studying how it should evaluate similar \u201c advanced driver assist \u201d systems .\n\u201c At some point , the question becomes : How much evidence is needed to determine that the way this technology is being used is unsafe ? \u201d said Jason Levine , executive director of the nonprofit Center for Auto Safety in Washington . \u201c In this instance , hopefully these tragedies will not be in vain and will lead to something more than an investigation by NHTSA . \u201d\nLevine and others have called on the agency to require Tesla to limit the use of Autopilot to mainly four-lane divided highways without cross traffic . They also want Tesla to install a better system to monitor drivers to make sure they \u2019 re paying attention all the time . Tesla \u2019 s system requires drivers to place their hands on the steering wheel . But federal investigators have found that this system lets drivers zone out for too long .\nTesla plans to use the same cameras and radar sensors , though with a more powerful computer , in its fully self-driving vehicles . Critics question whether those cars will be able to drive themselves safely without putting other motorists in danger .\nDoubts about Tesla \u2019 s Autopilot system have long persisted . In September , the National Transportation Safety Board , which investigates transportation accidents , issued a report saying that a design flaw in Autopilot and driver inattention combined to cause a Tesla Model S to slam into a firetruck parked along a Los Angeles-area freeway in January 2018 . The board determined that the driver was overly reliant on the system and that Autopilot \u2019 s design let him disengage from driving for too long .\nIn addition to the deaths on Sunday night , three U.S. fatal crashes since 2016 \u2014 two in Florida and one in Silicon Valley \u2014 involved vehicles using Autopilot .\nDavid Friedman , vice president of advocacy for Consumer Reports and a former acting NHTSA administrator , said the agency should have declared Autopilot defective and sought a recall after a 2016 crash in Florida that killed a driver . Neither Tesla \u2019 s system nor the driver had braked before the car went underneath a semi-trailer that had turned in front of the car .\n\u201c We don \u2019 t need any more people getting hurt for us to know that there is a problem and that Tesla and NHTSA have failed to address it , \u201d Friedman said .\nIn addition to NHTSA , states can regulate autonomous vehicles , though many have decided they want to encourage testing .\nIn the 2016 crash , NHTSA closed its investigation without seeking a recall . Friedman , who was not at NHTSA at the time , said the agency determined that the problem didn \u2019 t happen frequently . But he said that argument has since been debunked .\nFriedman said it \u2019 s foreseeable some drivers will not pay attention to the road while using Autopilot , so the system is defective .\n\u201c The public is owed some explanation for the lack of action , \u201d he said . \u201c Simply saying they \u2019 re continuing to investigate \u2014 that line has worn out its usefulness and its credibility . \u201d\nIn a statement , NHTSA said it relies on data to make decisions , and if it finds any vehicle poses an unreasonable safety risk , \u201c the agency will not hesitate to take action. \u201d NHTSA also has said it doesn \u2019 t want to stand in the way of technology given its life-saving potential .\nRaj Rajkumar , an electrical and computer engineering professor at Carnegie Mellon University , said it \u2019 s likely that the Tesla in Sunday \u2019 s California crash was operating on Autopilot , which has become confused in the past by lane lines . He speculated that the lane line was more visible for the exit ramp , so the car took the ramp because it looked like a freeway lane . He also suggested that the driver might not have been paying close attention .\n\u201c No normal human being would not slow down in an exit lane , \u201d he said .\nIn April , Musk said he expected to start converting the company \u2019 s electric cars to fully self-driving vehicles in 2020 to create a network of robotic taxis to compete against Uber and other ride-hailing services .\nAt the time , experts said the technology isn \u2019 t ready and that Tesla \u2019 s camera and radar sensors weren \u2019 t good enough for a self-driving system . Rajkumar and others say additional crashes have proved that to be true .\nMany experts say they \u2019 re not aware of fatal crashes involving similar driver-assist systems from General Motors , Mercedes and other automakers . GM monitors drivers with cameras and will shut down the driving system if they don \u2019 t watch the road .\nHe predicted more deaths involving Teslas if NHTSA fails to take action .",
    "content_original": "FILE - In this July 8, 2018, file photo, clouds are reflected above the company logo on the hood of a Tesla vehicle outside a showroom in Littleton, Colo. The National Highway Traffic Safety Administration is investigating the crash of a speeding Tesla that killed two people in a Los Angeles suburb, the agency announced Tuesday, Dec. 31, 2019. (AP Photo/David Zalubowski, File)\n\nFILE - In this July 8, 2018, file photo, clouds are reflected above the company logo on the hood of a Tesla vehicle outside a showroom in Littleton, Colo. The National Highway Traffic Safety Administration is investigating the crash of a speeding Tesla that killed two people in a Los Angeles suburb, the agency announced Tuesday, Dec. 31, 2019. (AP Photo/David Zalubowski, File)\n\nDETROIT (AP) \u2014 Three crashes involving Teslas that killed three people have increased scrutiny of the company\u2019s Autopilot driving system just months before CEO Elon Musk has planned to put fully self-driving cars on the streets.\n\nOn Sunday, a Tesla Model S sedan left a freeway in Gardena, California, at a high speed, ran a red light and struck a Honda Civic, killing two people inside, police said.\n\nOn the same day, a Tesla Model 3 hit a parked firetruck on an Indiana freeway, killing a passenger in the Tesla.\n\nAnd on Dec. 7, yet another Model 3 struck a police cruiser on a Connecticut highway, though no one was hurt.\n\nThe special crash investigation unit of the National Highway Traffic Safety Administration is looking into the California crash. The agency hasn\u2019t decided whether its special-crash unit will review the crash that occurred Sunday near Terre Haute, Indiana. In both cases, authorities have yet to determine whether Tesla\u2019s Autopilot system was being used.\n\nNHTSA also is investigating the Connecticut crash, in which the driver told police that the car was operating on Autopilot, a Tesla system designed to keep a car in its lane and a safe distance from other vehicles. Autopilot also can change lanes on its own.\n\nTesla has said repeatedly that its Autopilot system is designed only to assist drivers, who must still pay attention and be ready to intervene at all times. The company contends that Teslas with Autopilot are safer than vehicles without it, but cautions that the system does not prevent all crashes.\n\nEven so, experts and safety advocates say a string of Tesla crashes raises serious questions about whether drivers have become too reliant on Tesla\u2019s technology and whether the company does enough to ensure that drivers keep paying attention. Some critics have said it\u2019s past time for NHTSA to stop investigating and to take action, such as forcing Tesla to make sure drivers pay attention when the system is being used.\n\nNHTSA has started investigations into 13 Tesla crashes dating to at least 2016 in which the agency believes Autopilot was operating. The agency has yet to issue any regulations, though it is studying how it should evaluate similar \u201cadvanced driver assist\u201d systems.\n\n\u201cAt some point, the question becomes: How much evidence is needed to determine that the way this technology is being used is unsafe?\u201d said Jason Levine, executive director of the nonprofit Center for Auto Safety in Washington. \u201cIn this instance, hopefully these tragedies will not be in vain and will lead to something more than an investigation by NHTSA.\u201d\n\nLevine and others have called on the agency to require Tesla to limit the use of Autopilot to mainly four-lane divided highways without cross traffic. They also want Tesla to install a better system to monitor drivers to make sure they\u2019re paying attention all the time. Tesla\u2019s system requires drivers to place their hands on the steering wheel. But federal investigators have found that this system lets drivers zone out for too long.\n\nTesla plans to use the same cameras and radar sensors, though with a more powerful computer, in its fully self-driving vehicles. Critics question whether those cars will be able to drive themselves safely without putting other motorists in danger.\n\nDoubts about Tesla\u2019s Autopilot system have long persisted. In September, the National Transportation Safety Board, which investigates transportation accidents, issued a report saying that a design flaw in Autopilot and driver inattention combined to cause a Tesla Model S to slam into a firetruck parked along a Los Angeles-area freeway in January 2018. The board determined that the driver was overly reliant on the system and that Autopilot\u2019s design let him disengage from driving for too long.\n\nIn addition to the deaths on Sunday night, three U.S. fatal crashes since 2016 \u2014 two in Florida and one in Silicon Valley \u2014 involved vehicles using Autopilot.\n\nDavid Friedman, vice president of advocacy for Consumer Reports and a former acting NHTSA administrator, said the agency should have declared Autopilot defective and sought a recall after a 2016 crash in Florida that killed a driver. Neither Tesla\u2019s system nor the driver had braked before the car went underneath a semi-trailer that had turned in front of the car.\n\n\u201cWe don\u2019t need any more people getting hurt for us to know that there is a problem and that Tesla and NHTSA have failed to address it,\u201d Friedman said.\n\nIn addition to NHTSA, states can regulate autonomous vehicles, though many have decided they want to encourage testing.\n\nIn the 2016 crash, NHTSA closed its investigation without seeking a recall. Friedman, who was not at NHTSA at the time, said the agency determined that the problem didn\u2019t happen frequently. But he said that argument has since been debunked.\n\nFriedman said it\u2019s foreseeable some drivers will not pay attention to the road while using Autopilot, so the system is defective.\n\n\u201cThe public is owed some explanation for the lack of action,\u201d he said. \u201cSimply saying they\u2019re continuing to investigate \u2014 that line has worn out its usefulness and its credibility.\u201d\n\nIn a statement, NHTSA said it relies on data to make decisions, and if it finds any vehicle poses an unreasonable safety risk, \u201cthe agency will not hesitate to take action.\u201d NHTSA also has said it doesn\u2019t want to stand in the way of technology given its life-saving potential.\n\nMessages were left Thursday seeking comment from Tesla.\n\nRaj Rajkumar, an electrical and computer engineering professor at Carnegie Mellon University, said it\u2019s likely that the Tesla in Sunday\u2019s California crash was operating on Autopilot, which has become confused in the past by lane lines. He speculated that the lane line was more visible for the exit ramp, so the car took the ramp because it looked like a freeway lane. He also suggested that the driver might not have been paying close attention.\n\n\u201cNo normal human being would not slow down in an exit lane,\u201d he said.\n\nIn April, Musk said he expected to start converting the company\u2019s electric cars to fully self-driving vehicles in 2020 to create a network of robotic taxis to compete against Uber and other ride-hailing services.\n\nAt the time, experts said the technology isn\u2019t ready and that Tesla\u2019s camera and radar sensors weren\u2019t good enough for a self-driving system. Rajkumar and others say additional crashes have proved that to be true.\n\nMany experts say they\u2019re not aware of fatal crashes involving similar driver-assist systems from General Motors, Mercedes and other automakers. GM monitors drivers with cameras and will shut down the driving system if they don\u2019t watch the road.\n\n\u201cTesla is nowhere close to that standard,\u201d he said.\n\nHe predicted more deaths involving Teslas if NHTSA fails to take action.\n\n\u201cThis is very unfortunate,\u201d he said. \u201cJust tragic.\u201d",
    "source_url": "www.apnews.com",
    "bias_text": "center",
    "ID": "mw9wUnjn1J3EWBus"
}