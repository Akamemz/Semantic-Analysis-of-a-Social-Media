{
    "topic": "technology",
    "source": "Slate",
    "bias": 0,
    "url": "https://slate.com/technology/2019/08/facebook-anti-conservative-bias-audit-results-kyl.html",
    "title": "Facebook\u2019s Anti-Conservative Bias Audit Is Here",
    "date": "2019-08-21",
    "authors": "Renee Diresta",
    "content": "The preliminary results of Facebook \u2019 s long-awaited \u201c bias \u201d audit are out . The key takeaway ? Everyone is still unhappy . The report is little more than a formalized catalog of six categories of grievances aired in Republican-led congressional hearings over the past two years . It doesn \u2019 t include any real quantitative assessment of bias . There are no statistics assessing the millions of moderation decisions that Facebook and Instagram make each day . Instead , there are merely some capitulatory minor product tweaks to address edge cases , such as the permitting of images of premature babies to bolster pro-life ads ( previously , Facebook had apparently prohibited images of medical tubes connected to a human body ) .\nThese tiny changes are all the more remarkable because the audit was an exhaustive affair , the result of about a year of research led by former Republican Sen. Jon Kyl , encompassing interviews with scores of conservative lawmakers and organizations . Facebook committed to the audit in May 2018 , amid criticism that it silenced conservative voices .\nDespite the time and energy invested , the conspicuous absence of evidence within the audit suggests what many media researchers already knew : Allegations of political bias are political theater . Sen. Ted Cruz has been touting anecdotes about Silicon Valley censorship for more than a year . President Donald Trump has fundraised on it . Recognizing that it plays well , left-leaning politicians have begun to seize on the censorship talking point , too : Sen. Elizabeth Warren got angry about Facebook denying one of her ads ( it was later restored ) , and Rep. Tulsi Gabbard , another presidential candidate , is presently testing the limits of cognitive dissonance by suing Google for censorship while simultaneously touting her debate performance on Google search trends .\nStill , the audit findings ( or lack of them ) may help shift the conversation in a positive direction . While they \u2019 re unlikely to put a stop to the belief in political bias , perhaps they will dissuade the Trump administration from pursuing a misguided executive order to \u201c police \u201d social media censorship . That may be too optimistic . But perhaps the findings\u2014and the challenges of even conducting a meaningful audit\u2014could be used to focus the conversation on real problems with social media : an advertising infrastructure masquerading as a communications infrastructure and algorithms that incentivize misinformation .\nViral stories of individual bad calls spread like wildfire , particularly if they feed the narrative that a distinct group is being censored .\nFewer than two weeks ago , a draft executive order leaked , detailing a plan by Trump to address \u201c anti-conservative bias \u201d on social media platforms , including Facebook and Twitter . ( The leak came shortly after a bizarre \u201c social media summit \u201d held at the White House . ) The order , titled \u201c Protecting Americans From Online Censorship , \u201d signaled an effort to take the topic of anti-conservative bias from sound bites to rule-making . If enacted , it could significantly change the long-standing law that has governed American speech online and make major tech platforms liable for perceived censorship . The order would seek to grant the Federal Communications Commission and Federal Trade Commission new power over the content that social media companies currently moderate themselves .\nBut the volume of content and moderation decisions that those agencies would face is staggering : From July to December 2018 , Twitter \u2019 s transparency report notes that 11 million unique accounts were reported for rules violations . Of those , Twitter took some sort of action on 235,455 flagged as abuse , 250,806 for hateful conduct , and 56,577 for violent threats . Meanwhile , Facebook moderates millions of posts per week . In a three-month period at the start of 2019 , Facebook took action on 2.6 million pieces of content related to harassment and 4 million related to hate speech . Given that volume , human moderators and algorithmic systems inevitably make bad calls . The problem is that viral stories of those individual bad calls spread like wildfire , particularly if they feed the narrative that a distinct group is being censored .\nThe topic of free speech is always thorny , but the evidence that is supposed to support conservative criticism broadly , and Trump \u2019 s draft order specifically , simply doesn \u2019 t stand up to scrutiny . First , using the metric that is most transparent\u2014reach\u2014conservative news is top-performing content on Facebook . Attempts to take a quantitative approach to assessing conservative censorship , such as a Quillette article analyzing prominent Twitter accounts that were banned , revealed something else entirely : \u201c Of 22 prominent , politically active individuals who are known to have been suspended since 2005 and who expressed a preference in the 2016 U.S. presidential election , 21 supported Donald Trump , \u201d the author assessed . Among the 21 were conservative paragons Tila Tequila , David Duke , and the American Nazi Party ; other conservatives sharply critiqued the attempt to reclassify extremists as conservatives . Even the anecdotes are misleading : One of the most touted is the story of Rep. ( now-Sen. ) Marsha Blackburn , who alleged conservative censorship when she was prevented from running an ad that made false claims about Planned Parenthood selling baby parts . Twitter initially declined to allow what it deemed a contested and inflammatory claim to be served as an ad but said the Blackburn campaign was free to post and promote it organically on the site .\nFor many researchers who study social media and moderation , the hope was that the conservative bias audit would provide a large-scale assessment that would resolve the question of systemic political bias . It didn \u2019 t happen . And that \u2019 s partially because it can \u2019 t . Ultimately , this is a system of incredible complexity in which moderators make millions of hard calls . It \u2019 s also a system with no transparency or oversight . It \u2019 s time for partisans to stop working the refs to achieve better outcomes for their parties and instead work toward bipartisan policies that benefit us all .\nWe all spend time on a communications infrastructure that \u2019 s opaque , controlled by a handful of companies , and built to drive engagement at any cost . Feelings of frustration emerge when moderation rules are applied unfairly and there \u2019 s no one to appeal to . These are problems the left and right can find common ground on . Lawmakers and pundits in both parties should push for more transparency into recommendation engines and content takedowns , more accountability when those algorithms promote dangerous content , and a common-sense appeals process when individuals believe they have been unfairly censored .\nThere are already bright ideas in this vein : The social media industry and civil society could identify a system of shared standards or best practices ; companies could then decide whether to adopt those standards , and consumers could sign up for platforms that best suited them . Improved A.I . moderation , coupled with a robust human appeals process and subject to auditing , is also a way forward\u2014and would reduce the harms that many human moderators experience . There \u2019 s also the potential to replicate and improve upon systems such as the Lumen database , which makes copyright-related content takedowns visible to outside researchers ; this would increase transparency and enable ongoing assessments to ensure there \u2019 s no bias .\nWe all want an internet that facilitates free expression . It \u2019 s time to pursue ideas that get us there , rather than political orders masquerading as policy .\nFuture Tense is a partnership of \u2588\u2588\u2588 , New America , and Arizona State University that examines emerging technologies , public policy , and society .",
    "content_original": "Former Republican Sen. Jon Kyl led the exhaustive audit. Photo illustration by Slate. Images by Facebook and Photo by Zach Gibson/Getty Images.\n\nThe preliminary results of Facebook\u2019s long-awaited \u201cbias\u201d audit are out. The key takeaway? Everyone is still unhappy. The report is little more than a formalized catalog of six categories of grievances aired in Republican-led congressional hearings over the past two years. It doesn\u2019t include any real quantitative assessment of bias. There are no statistics assessing the millions of moderation decisions that Facebook and Instagram make each day. Instead, there are merely some capitulatory minor product tweaks to address edge cases, such as the permitting of images of premature babies to bolster pro-life ads (previously, Facebook had apparently prohibited images of medical tubes connected to a human body).\n\nThese tiny changes are all the more remarkable because the audit was an exhaustive affair, the result of about a year of research led by former Republican Sen. Jon Kyl, encompassing interviews with scores of conservative lawmakers and organizations. Facebook committed to the audit in May 2018, amid criticism that it silenced conservative voices.\n\nDespite the time and energy invested, the conspicuous absence of evidence within the audit suggests what many media researchers already knew: Allegations of political bias are political theater. Sen. Ted Cruz has been touting anecdotes about Silicon Valley censorship for more than a year. President Donald Trump has fundraised on it. Recognizing that it plays well, left-leaning politicians have begun to seize on the censorship talking point, too: Sen. Elizabeth Warren got angry about Facebook denying one of her ads (it was later restored), and Rep. Tulsi Gabbard, another presidential candidate, is presently testing the limits of cognitive dissonance by suing Google for censorship while simultaneously touting her debate performance on Google search trends.\n\nStill, the audit findings (or lack of them) may help shift the conversation in a positive direction. While they\u2019re unlikely to put a stop to the belief in political bias, perhaps they will dissuade the Trump administration from pursuing a misguided executive order to \u201cpolice\u201d social media censorship. That may be too optimistic. But perhaps the findings\u2014and the challenges of even conducting a meaningful audit\u2014could be used to focus the conversation on real problems with social media: an advertising infrastructure masquerading as a communications infrastructure and algorithms that incentivize misinformation.\n\nViral stories of individual bad calls spread like wildfire, particularly if they feed the narrative that a distinct group is being censored.\n\nFewer than two weeks ago, a draft executive order leaked, detailing a plan by Trump to address \u201canti-conservative bias\u201d on social media platforms, including Facebook and Twitter. (The leak came shortly after a bizarre \u201csocial media summit\u201d held at the White House.) The order, titled \u201cProtecting Americans From Online Censorship,\u201d signaled an effort to take the topic of anti-conservative bias from sound bites to rule-making. If enacted, it could significantly change the long-standing law that has governed American speech online and make major tech platforms liable for perceived censorship. The order would seek to grant the Federal Communications Commission and Federal Trade Commission new power over the content that social media companies currently moderate themselves.\n\nBut the volume of content and moderation decisions that those agencies would face is staggering: From July to December 2018, Twitter\u2019s transparency report notes that 11 million unique accounts were reported for rules violations. Of those, Twitter took some sort of action on 235,455 flagged as abuse, 250,806 for hateful conduct, and 56,577 for violent threats. Meanwhile, Facebook moderates millions of posts per week. In a three-month period at the start of 2019, Facebook took action on 2.6 million pieces of content related to harassment and 4 million related to hate speech. Given that volume, human moderators and algorithmic systems inevitably make bad calls. The problem is that viral stories of those individual bad calls spread like wildfire, particularly if they feed the narrative that a distinct group is being censored.\n\nThe topic of free speech is always thorny, but the evidence that is supposed to support conservative criticism broadly, and Trump\u2019s draft order specifically, simply doesn\u2019t stand up to scrutiny. First, using the metric that is most transparent\u2014reach\u2014conservative news is top-performing content on Facebook. Attempts to take a quantitative approach to assessing conservative censorship, such as a Quillette article analyzing prominent Twitter accounts that were banned, revealed something else entirely: \u201cOf 22 prominent, politically active individuals who are known to have been suspended since 2005 and who expressed a preference in the 2016 U.S. presidential election, 21 supported Donald Trump,\u201d the author assessed. Among the 21 were conservative paragons Tila Tequila, David Duke, and the American Nazi Party; other conservatives sharply critiqued the attempt to reclassify extremists as conservatives. Even the anecdotes are misleading: One of the most touted is the story of Rep. (now-Sen.) Marsha Blackburn, who alleged conservative censorship when she was prevented from running an ad that made false claims about Planned Parenthood selling baby parts. Twitter initially declined to allow what it deemed a contested and inflammatory claim to be served as an ad but said the Blackburn campaign was free to post and promote it organically on the site.\n\nFor many researchers who study social media and moderation, the hope was that the conservative bias audit would provide a large-scale assessment that would resolve the question of systemic political bias. It didn\u2019t happen. And that\u2019s partially because it can\u2019t. Ultimately, this is a system of incredible complexity in which moderators make millions of hard calls. It\u2019s also a system with no transparency or oversight. It\u2019s time for partisans to stop working the refs to achieve better outcomes for their parties and instead work toward bipartisan policies that benefit us all.\n\nWe all spend time on a communications infrastructure that\u2019s opaque, controlled by a handful of companies, and built to drive engagement at any cost. Feelings of frustration emerge when moderation rules are applied unfairly and there\u2019s no one to appeal to. These are problems the left and right can find common ground on. Lawmakers and pundits in both parties should push for more transparency into recommendation engines and content takedowns, more accountability when those algorithms promote dangerous content, and a common-sense appeals process when individuals believe they have been unfairly censored.\n\nThere are already bright ideas in this vein: The social media industry and civil society could identify a system of shared standards or best practices; companies could then decide whether to adopt those standards, and consumers could sign up for platforms that best suited them. Improved A.I. moderation, coupled with a robust human appeals process and subject to auditing, is also a way forward\u2014and would reduce the harms that many human moderators experience. There\u2019s also the potential to replicate and improve upon systems such as the Lumen database, which makes copyright-related content takedowns visible to outside researchers; this would increase transparency and enable ongoing assessments to ensure there\u2019s no bias.\n\nWe all want an internet that facilitates free expression. It\u2019s time to pursue ideas that get us there, rather than political orders masquerading as policy.\n\nFuture Tense is a partnership of Slate, New America, and Arizona State University that examines emerging technologies, public policy, and society.",
    "source_url": "www.slate.com",
    "bias_text": "left",
    "ID": "g3FEf7fwZpUijf8E"
}