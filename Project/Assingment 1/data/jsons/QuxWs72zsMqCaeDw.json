{
    "topic": "technology",
    "source": "Vox",
    "bias": 0,
    "url": "https://www.vox.com/policy-and-politics/2019/3/17/18269617/new-zealand-shooting-mosque-online-extremism-tech",
    "title": "Why tech companies failed to keep the New Zealand shooter\u2019s extremism from going viral",
    "date": "2019-03-17",
    "authors": "Amanda Sakuma, Alissa Wilkinson, Sigal Samuel, Terry Nguyen, Rebecca Jennings, Hannah Brown, Lauren Katz",
    "content": "The hate-filled terror rampage at two mosques in Christchurch , New Zealand , was meticulously designed to maximize the number of witnesses around the globe , highlighting the difficulty in putting a lid on extremist hate that spreads online .\nThe suspected gunman did everything he could to make his shooting spree go viral . He live-streamed the attack on social media , wearing a body camera to simulate a video game . He shared a rambling 74-page manifesto espousing white supremacy that was full of memes and easter eggs meant to invite attention from all corners of the internet and admiration from other extremists who live online . The shooter had laid a trap across the internet that exploited the newsworthiness of the attack and leaned into peoples \u2019 inclination to gawk at horror and violence . Even professional journalistic institutions gave in to the temptation to air video of the massacre .\nScrubbing the video from the internet was like playing a game of whack-a-mole . Facebook quickly removed the alleged gunman \u2019 s Facebook and Instagram accounts \u2014 but not because its algorithm or moderators had flagged the violent content in real time . New Zealand authorities had to ask for the video to be taken down . Internet service providers in New Zealand rushed to \u201c close off \u201d websites that were distributing the video , but then a number of copy-cat sites immediately started popping up .\nIt soon didn \u2019 t matter that the original video was removed . The clip had already been downloaded and re-upped online faster than tech companies could respond . Facebook alone says it removed 1.5 million videos within the first 24 hours of the attack . And those are just the clips they were able to catch .\nFriday \u2019 s massacre exemplified a larger problem that \u2019 s plaguing the internet . Platforms are struggling to self-police problematic content created by its users , while the lawmakers who would ostensibly impose regulations are either too reluctant or ill-equipped to do so \u2014 and many in both camps are predisposed to treat far-right rhetoric less seriously than other forms of extremism , to boot .\nAs the death toll rises \u2014 now 50 lives have been taken since Friday \u2019 s shooting , making it one of the deadliest terror attacks carried out by a far-right extremist in recent memory \u2014 the attack adds extra weight to the question that tech companies , policymakers , and social media users have been asking : How do you effectively police online hate ?\nThe shooter \u2019 s viral video outpaced social media company \u2019 s content moderation\nThe world \u2019 s largest tech companies were forced to scramble on Friday to keep the violent screed from spreading . Facebook said it was removing any praise or support of the shooting , and had a process to flag the digital fingerprint of disturbing materials . YouTube said it was \u201c working vigilantly \u201d to remove violent footage , while Twitter said it suspended the account that posted the original video . Reddit on Friday eventually resorted to taking down two infamous subreddits , r/watchpeopledie and r/gory .\nDespite those efforts , videos of the attack were easy to find through simple searches online , even hours and days after the initial shooting spree . The swift dissemination highlights how ill-equipped tech companies remain in addressing the vile , racist , and excessively violent content that \u2019 s being shared on their platforms .\nTook me about 30 seconds to find YouTube videos of the ripped livestream : pic.twitter.com/TFkQHIqQbf \u2014 Jason Abbruzzese ( @ JasonAbbruzzese ) March 15 , 2019\nModerators already face an uphill battle in keeping offensive and violent content offline ; the Christchurch terror attack shows the difficulty of catching deeply problematic video live-streams in real time .\nFor one , it \u2019 s generally easier for software to scan text and offensive comments as opposed to moving images in a video . But even when the technical tools exist , policing-breaking news poses unique problems . YouTube , for example , does have a system for automatically removing copyrighted content or prohibited materials , and told the Verge \u2019 s Julia Alexander that any exact re-uploads of the alleged shooter \u2019 s videos would be automatically deleted . But the algorithm can \u2019 t be used to tamp down on edited versions of the Christchurch shooting , because Youtube wants to \u201c ensure that news videos that use a portion of the video for their segments aren \u2019 t removed in the process \u201d :\nYouTube \u2019 s safety team thinks of it as a balancing act , according to sources familiar with their thinking . For major news events like yesterday \u2019 s shooting , YouTube \u2019 s team uses a system that \u2019 s similar to its copyright tool , Content ID , but not exactly the same . It searches re-uploaded versions of the original video for similar metadata and imagery . If it \u2019 s an unedited re-upload , it \u2019 s removed . If it \u2019 s edited , the tool flags it to a team of human moderators , both full-time employees at YouTube and contractors , who determine if the video violates the company \u2019 s policies .\nThat process is not just traumatizing for the individual moderators who are forced to watch the horrific footage , it \u2019 s also an imperfect system to limit its reach \u2014 particularly in a fast-moving event like Friday \u2019 s tragedy .\nTech companies are expected to self-police . So far , they \u2019 re falling short .\nAt this point , in theory , tech companies should be well-practiced in the art of blocking far-right hate speech and violence from their platforms . They \u2019 ve been having to deal with it for years .\nAfter the 2017 Unite the Right rally of neo-Nazis and white supremacists in Charlottesville , Virginia \u2014 where a woman was mowed down and killed by an avowed Nazi sympathizer \u2014 tech companies faced intense public pressure to block prominent instigators of explicit far-right extremism . Twitter suspended a bunch of white supremacists and prominent provocateurs \u2014 including Milo Yiannoppolis , Alex Jones , and Gavin McInnes \u2014 but was hesitant to target other alt-right leaders like Richard Spencer . Gab and the Daily Stormer , two havens for neo-Nazis , were similarly banished to the darker recesses of the internet . Reddit quarantined hate-fueled subreddits , while other companies like PayPal , GoDaddy , and Squarespace blocked white supremacists from using their services .\nIn effect , individual leaders and groups were targeted in response to a high-profile flashpoint in American politics and culture . But for many critics , those actions were hollow in addressing the underlying proliferation of racist and white supremacist ideas that are peddled online .\nAnd even minimal efforts at reform have come with costs for the social media giants \u2014 big ones . As \u2588\u2588\u2588 \u2019 s Emily Stewart noted after Facebook \u2019 s stock saw the biggest one-day drop in history last fall ( with $ 119 billion wiped off of its value after the company reported slower-than-expected revenue growth ) , social media companies \u2019 efforts to address issues with their platforms garner \u201c enormous backlash from Wall Street . \u201d\nThe message from investors is clear : They \u2019 re nervous about what bad headlines and subsequent changes from social media platforms could do to their bottom lines . If Twitter and Facebook police their sites in a way that affects engagement or cracks down on content , or if privacy controls that ask users to opt in to their data being shared lead to more of them opting out , ad dollars could fall . And hiring workers to increase privacy protections and monitor activity is expensive . ... This week offers a lesson we don \u2019 t necessarily want executives to take away : try to be better , and potentially be severely punished by investors .\nMany companies only start to take action on long-standing issues when the financial risks of not doing anything become higher than the likely costs they \u2019 ll encounter .\nYouTube , for example , is under fire for failing to adequately combat conspiracies and prevent child exploitation from being circulated . Its algorithm has a troubling record of surfacing and recommending content that violates its own policies . Major advertisers \u2014including Disney and Nestle \u2014 started to bolt earlier this year after finding that their ads were appearing in videos full of offensive and sexually explicit comments aimed at children . In response , YouTube purged hundreds of its users and said it would change the way new videos are elevated and surfaced , following up on a crackdown in 2017 from reports that videos full of predatory comments were being recommended to kids .\nSome lawmakers are growing impatient with tech companies \u2019 self-regulation \u2014 but it \u2019 s not clear they can do it any better\nEven as platforms have tried to regulate themselves in recent years , some policymakers \u2019 patience for letting them do so is growing short . But the legislative solutions some of them have proposed \u2014 or lack thereof \u2014 also struggle to match the pace of change in internet culture and the communities that foster extremist ideas and behaviors .\nCongress so far has struggled to grapple with \u2014 or even understand \u2014 the many tentacles of problems plaguing social networks , from tackling the spread of misinformation to regulating how sites handle user data and privacy .\nSome members of Congress have been woefully ill-prepared to even talk about tech issues ( during one hearing last year , a lawmaker asked the Google CEO questions about his iPhone ) . And even when they are interested and equipped to talk about regulating the internet , many US lawmakers have been \u201c reticent to clamp down at the risk of harming growth , \u201d Stewart noted :\nIn a Senate hearing in April , Sen. Orrin Hatch ( R-UT ) asked Zuckerberg what \u201c sorts of legislative changes \u201d he thought should be enacted to prevent a Cambridge Analytica repeat . Sen. Lindsey Graham ( R-SC ) , who also pressed Zuckerberg on whether Facebook is a monopoly , asked the executive to submit some proposed regulations to him .\nStill , interest is growing . In the 2020 presidential primary race , Democratic candidates have vowed to take on Big Tech \u2014 Sen. Elizabeth Warren has gone as far as proposing to break up Google , Facebook , and Amazon , while Sen. Amy Klobuchar is expected to make tech reform a banner issue for her campaign .\nThere \u2019 s a growing appetite for reform elsewhere in the world . The European Union took a stand on privacy concerns with General Data Protection Regulation Act , or GDPR , a law enacted last year to compel transparency around the data that companies collect and how it is used . And now some countries want to crack down on extremist content , too .\nA British Parliamentary committee wants Facebook to be held legally liable for the content posted on the platform . The legislative body recently wrapped up an 18-month investigation into the social media site , finding that it violated data privacy and competition laws . And in the wake of the Christchurch terror attacks , British officials are threatening that tech companies be \u201c prepared to face the force of the law \u201d if they don \u2019 t put a lid on the spread of hateful messages .\nYou really need to do more @ YouTube @ Google @ facebook @ Twitter to stop violent extremism being promoted on your platforms . Take some ownership . Enough is enough https : //t.co/GTSgRufOow \u2014 Sajid Javid ( @ sajidjavid ) March 15 , 2019\nThe response to Islamic extremism online is often treated much differently than white supremacy\nIt \u2019 s well documented that social media has played an important role in helping fuel extremism and hate . Just look to the spread of ISIS , which notoriously leveraged and exploited platforms to recruit new members and promote propaganda . But more often than not , US authorities focus on Islamic extremism , even as homegrown right-wing terror has begun to have its moment .\nThat holds true for the tech companies as well . Even as they worked up solutions to combat ISIS online , they \u2019 ve been flat-footed in their response to white nationalism and white supremacy . Last year , Motherboard found that while YouTube was cracking down on videos of ISIS recruits , footage promoting neo-Nazi propaganda stayed online for months and even years .\nAnd when researchers from Program on Extremism at George Washington University compared far-right extremism with ISIS online behavior , they found that the growth in white nationalist movements outpaced Islamic extremism by virtually every metric .\nThe white nationalist datasets examined outperformed ISIS in most current metrics and many historical metrics . White nationalists and Nazis had substantially higher follower counts than ISIS supporters , and tweeted more often . ISIS supporters had better discipline regarding consistent use of the movement \u2019 s hashtags , but trailed in virtually every other respect . The clear advantage enjoyed by white nationalists was attributable in part to the effects of aggressive suspensions of accounts associated with ISIS networks .\nPart of that could be the difficulty companies face in identifying offensive far-right content . As seen with the Christchurch manifesto , far-right extremism has a unique life online with its own language that \u2019 s embedded in memes and \u201c shitposts \u201d and is difficult to decipher . As \u2588\u2588\u2588 \u2019 s Aja Romano outlines in an rundown of the manifesto \u2019 s underlying message , the alt-right has mastered the art of online trolling to \u201c distort what their actual message is , so they can claim plausible deniability that their message is harmful or bad . \u201d\nBut leaving it unchecked has consequences : The surge in online activity coincides with a rise in real-world hate , particularly in the US . One study found that the number of far-right terror attacks in America more than quadrupled over the first year of Donald Trump \u2019 s presidency .\nIn the last year alone , there have been a number of high-profile flare-ups of far-right violence . A US Coast Guard and self-proclaimed white nationalist had stockpiled weapons and ammunition with plans to stage an attack targeting Democratic politicians , journalists , and judges . Last fall \u2019 s Pittsburgh shooting targeting Jews at the Tree of Life synagogue left 11 dead . In October , a man sent 13 pipe bombs to prominent Democrats and critics of Trump .\nNone of those incidents prompted major reform efforts on tech companies \u2019 parts . But in light of the graphic massacre in New Zealand , there \u2019 s a chance the conversation around right-wing extremism may change . The staggering violence of ISIS \u2019 s campaign helped define it as a terror-driven organization and made tech companies and governments alike get serious about combatting its propaganda online . Are they prepared to do the same with white supremacy ?\nThe Christchurch shooter livestreamed his attack . The video was disseminated across the internet even as platforms desperately worked to remove it . \u2014 Ellie Hall ( @ ellievhall ) March 16 , 2019",
    "content_original": "The hate-filled terror rampage at two mosques in Christchurch, New Zealand, was meticulously designed to maximize the number of witnesses around the globe, highlighting the difficulty in putting a lid on extremist hate that spreads online.\n\nThe suspected gunman did everything he could to make his shooting spree go viral. He live-streamed the attack on social media, wearing a body camera to simulate a video game. He shared a rambling 74-page manifesto espousing white supremacy that was full of memes and easter eggs meant to invite attention from all corners of the internet and admiration from other extremists who live online. The shooter had laid a trap across the internet that exploited the newsworthiness of the attack and leaned into peoples\u2019 inclination to gawk at horror and violence. Even professional journalistic institutions gave in to the temptation to air video of the massacre.\n\nScrubbing the video from the internet was like playing a game of whack-a-mole. Facebook quickly removed the alleged gunman\u2019s Facebook and Instagram accounts \u2014 but not because its algorithm or moderators had flagged the violent content in real time. New Zealand authorities had to ask for the video to be taken down. Internet service providers in New Zealand rushed to \u201cclose off\u201d websites that were distributing the video, but then a number of copy-cat sites immediately started popping up.\n\nIt soon didn\u2019t matter that the original video was removed. The clip had already been downloaded and re-upped online faster than tech companies could respond. Facebook alone says it removed 1.5 million videos within the first 24 hours of the attack. And those are just the clips they were able to catch.\n\nFriday\u2019s massacre exemplified a larger problem that\u2019s plaguing the internet. Platforms are struggling to self-police problematic content created by its users, while the lawmakers who would ostensibly impose regulations are either too reluctant or ill-equipped to do so \u2014 and many in both camps are predisposed to treat far-right rhetoric less seriously than other forms of extremism, to boot.\n\nAs the death toll rises \u2014 now 50 lives have been taken since Friday\u2019s shooting, making it one of the deadliest terror attacks carried out by a far-right extremist in recent memory \u2014 the attack adds extra weight to the question that tech companies, policymakers, and social media users have been asking: How do you effectively police online hate?\n\nThe shooter\u2019s viral video outpaced social media company\u2019s content moderation\n\nThe world\u2019s largest tech companies were forced to scramble on Friday to keep the violent screed from spreading. Facebook said it was removing any praise or support of the shooting, and had a process to flag the digital fingerprint of disturbing materials. YouTube said it was \u201cworking vigilantly\u201d to remove violent footage, while Twitter said it suspended the account that posted the original video. Reddit on Friday eventually resorted to taking down two infamous subreddits, r/watchpeopledie and r/gory.\n\nDespite those efforts, videos of the attack were easy to find through simple searches online, even hours and days after the initial shooting spree. The swift dissemination highlights how ill-equipped tech companies remain in addressing the vile, racist, and excessively violent content that\u2019s being shared on their platforms.\n\nTook me about 30 seconds to find YouTube videos of the ripped livestream: pic.twitter.com/TFkQHIqQbf \u2014 Jason Abbruzzese (@JasonAbbruzzese) March 15, 2019\n\nModerators already face an uphill battle in keeping offensive and violent content offline; the Christchurch terror attack shows the difficulty of catching deeply problematic video live-streams in real time.\n\nFor one, it\u2019s generally easier for software to scan text and offensive comments as opposed to moving images in a video. But even when the technical tools exist, policing-breaking news poses unique problems. YouTube, for example, does have a system for automatically removing copyrighted content or prohibited materials, and told the Verge\u2019s Julia Alexander that any exact re-uploads of the alleged shooter\u2019s videos would be automatically deleted. But the algorithm can\u2019t be used to tamp down on edited versions of the Christchurch shooting, because Youtube wants to \u201censure that news videos that use a portion of the video for their segments aren\u2019t removed in the process\u201d:\n\nYouTube\u2019s safety team thinks of it as a balancing act, according to sources familiar with their thinking. For major news events like yesterday\u2019s shooting, YouTube\u2019s team uses a system that\u2019s similar to its copyright tool, Content ID, but not exactly the same. It searches re-uploaded versions of the original video for similar metadata and imagery. If it\u2019s an unedited re-upload, it\u2019s removed. If it\u2019s edited, the tool flags it to a team of human moderators, both full-time employees at YouTube and contractors, who determine if the video violates the company\u2019s policies.\n\nThat process is not just traumatizing for the individual moderators who are forced to watch the horrific footage, it\u2019s also an imperfect system to limit its reach \u2014 particularly in a fast-moving event like Friday\u2019s tragedy.\n\nTech companies are expected to self-police. So far, they\u2019re falling short.\n\nAt this point, in theory, tech companies should be well-practiced in the art of blocking far-right hate speech and violence from their platforms. They\u2019ve been having to deal with it for years.\n\nAfter the 2017 Unite the Right rally of neo-Nazis and white supremacists in Charlottesville, Virginia \u2014 where a woman was mowed down and killed by an avowed Nazi sympathizer \u2014 tech companies faced intense public pressure to block prominent instigators of explicit far-right extremism. Twitter suspended a bunch of white supremacists and prominent provocateurs \u2014 including Milo Yiannoppolis, Alex Jones, and Gavin McInnes \u2014 but was hesitant to target other alt-right leaders like Richard Spencer. Gab and the Daily Stormer, two havens for neo-Nazis, were similarly banished to the darker recesses of the internet. Reddit quarantined hate-fueled subreddits, while other companies like PayPal, GoDaddy, and Squarespace blocked white supremacists from using their services.\n\nIn effect, individual leaders and groups were targeted in response to a high-profile flashpoint in American politics and culture. But for many critics, those actions were hollow in addressing the underlying proliferation of racist and white supremacist ideas that are peddled online.\n\nAnd even minimal efforts at reform have come with costs for the social media giants \u2014 big ones. As Vox\u2019s Emily Stewart noted after Facebook\u2019s stock saw the biggest one-day drop in history last fall (with $119 billion wiped off of its value after the company reported slower-than-expected revenue growth), social media companies\u2019 efforts to address issues with their platforms garner \u201cenormous backlash from Wall Street.\u201d\n\nThe message from investors is clear: They\u2019re nervous about what bad headlines and subsequent changes from social media platforms could do to their bottom lines. If Twitter and Facebook police their sites in a way that affects engagement or cracks down on content, or if privacy controls that ask users to opt in to their data being shared lead to more of them opting out, ad dollars could fall. And hiring workers to increase privacy protections and monitor activity is expensive. ... This week offers a lesson we don\u2019t necessarily want executives to take away: try to be better, and potentially be severely punished by investors.\n\nMany companies only start to take action on long-standing issues when the financial risks of not doing anything become higher than the likely costs they\u2019ll encounter.\n\nYouTube, for example, is under fire for failing to adequately combat conspiracies and prevent child exploitation from being circulated. Its algorithm has a troubling record of surfacing and recommending content that violates its own policies. Major advertisers \u2014including Disney and Nestle \u2014 started to bolt earlier this year after finding that their ads were appearing in videos full of offensive and sexually explicit comments aimed at children. In response, YouTube purged hundreds of its users and said it would change the way new videos are elevated and surfaced, following up on a crackdown in 2017 from reports that videos full of predatory comments were being recommended to kids.\n\nSome lawmakers are growing impatient with tech companies\u2019 self-regulation \u2014 but it\u2019s not clear they can do it any better\n\nEven as platforms have tried to regulate themselves in recent years, some policymakers\u2019 patience for letting them do so is growing short. But the legislative solutions some of them have proposed \u2014 or lack thereof \u2014 also struggle to match the pace of change in internet culture and the communities that foster extremist ideas and behaviors.\n\nCongress so far has struggled to grapple with \u2014 or even understand \u2014 the many tentacles of problems plaguing social networks, from tackling the spread of misinformation to regulating how sites handle user data and privacy.\n\nSome members of Congress have been woefully ill-prepared to even talk about tech issues (during one hearing last year, a lawmaker asked the Google CEO questions about his iPhone). And even when they are interested and equipped to talk about regulating the internet, many US lawmakers have been \u201creticent to clamp down at the risk of harming growth,\u201d Stewart noted:\n\nIn a Senate hearing in April, Sen. Orrin Hatch (R-UT) asked Zuckerberg what \u201csorts of legislative changes\u201d he thought should be enacted to prevent a Cambridge Analytica repeat. Sen. Lindsey Graham (R-SC), who also pressed Zuckerberg on whether Facebook is a monopoly, asked the executive to submit some proposed regulations to him.\n\nStill, interest is growing. In the 2020 presidential primary race, Democratic candidates have vowed to take on Big Tech \u2014 Sen. Elizabeth Warren has gone as far as proposing to break up Google, Facebook, and Amazon, while Sen. Amy Klobuchar is expected to make tech reform a banner issue for her campaign.\n\nThere\u2019s a growing appetite for reform elsewhere in the world. The European Union took a stand on privacy concerns with General Data Protection Regulation Act, or GDPR, a law enacted last year to compel transparency around the data that companies collect and how it is used. And now some countries want to crack down on extremist content, too.\n\nA British Parliamentary committee wants Facebook to be held legally liable for the content posted on the platform. The legislative body recently wrapped up an 18-month investigation into the social media site, finding that it violated data privacy and competition laws. And in the wake of the Christchurch terror attacks, British officials are threatening that tech companies be \u201cprepared to face the force of the law\u201d if they don\u2019t put a lid on the spread of hateful messages.\n\nYou really need to do more @YouTube @Google @facebook @Twitter to stop violent extremism being promoted on your platforms. Take some ownership. Enough is enough https://t.co/GTSgRufOow \u2014 Sajid Javid (@sajidjavid) March 15, 2019\n\nThe response to Islamic extremism online is often treated much differently than white supremacy\n\nIt\u2019s well documented that social media has played an important role in helping fuel extremism and hate. Just look to the spread of ISIS, which notoriously leveraged and exploited platforms to recruit new members and promote propaganda. But more often than not, US authorities focus on Islamic extremism, even as homegrown right-wing terror has begun to have its moment.\n\nThat holds true for the tech companies as well. Even as they worked up solutions to combat ISIS online, they\u2019ve been flat-footed in their response to white nationalism and white supremacy. Last year, Motherboard found that while YouTube was cracking down on videos of ISIS recruits, footage promoting neo-Nazi propaganda stayed online for months and even years.\n\nAnd when researchers from Program on Extremism at George Washington University compared far-right extremism with ISIS online behavior, they found that the growth in white nationalist movements outpaced Islamic extremism by virtually every metric.\n\nThe white nationalist datasets examined outperformed ISIS in most current metrics and many historical metrics. White nationalists and Nazis had substantially higher follower counts than ISIS supporters, and tweeted more often. ISIS supporters had better discipline regarding consistent use of the movement\u2019s hashtags, but trailed in virtually every other respect. The clear advantage enjoyed by white nationalists was attributable in part to the effects of aggressive suspensions of accounts associated with ISIS networks.\n\nPart of that could be the difficulty companies face in identifying offensive far-right content. As seen with the Christchurch manifesto, far-right extremism has a unique life online with its own language that\u2019s embedded in memes and \u201cshitposts\u201d and is difficult to decipher. As Vox\u2019s Aja Romano outlines in an rundown of the manifesto\u2019s underlying message, the alt-right has mastered the art of online trolling to \u201cdistort what their actual message is, so they can claim plausible deniability that their message is harmful or bad.\u201d\n\nBut leaving it unchecked has consequences: The surge in online activity coincides with a rise in real-world hate, particularly in the US. One study found that the number of far-right terror attacks in America more than quadrupled over the first year of Donald Trump\u2019s presidency.\n\nIn the last year alone, there have been a number of high-profile flare-ups of far-right violence. A US Coast Guard and self-proclaimed white nationalist had stockpiled weapons and ammunition with plans to stage an attack targeting Democratic politicians, journalists, and judges. Last fall\u2019s Pittsburgh shooting targeting Jews at the Tree of Life synagogue left 11 dead. In October, a man sent 13 pipe bombs to prominent Democrats and critics of Trump.\n\nNone of those incidents prompted major reform efforts on tech companies\u2019 parts. But in light of the graphic massacre in New Zealand, there\u2019s a chance the conversation around right-wing extremism may change. The staggering violence of ISIS\u2019s campaign helped define it as a terror-driven organization and made tech companies and governments alike get serious about combatting its propaganda online. Are they prepared to do the same with white supremacy?\n\nThe Christchurch shooter livestreamed his attack. The video was disseminated across the internet even as platforms desperately worked to remove it. \u2014 Ellie Hall (@ellievhall) March 16, 2019",
    "source_url": "www.vox.com",
    "bias_text": "left",
    "ID": "QuxWs72zsMqCaeDw"
}