{
    "topic": "media_bias",
    "source": "NPR Online News",
    "bias": 1,
    "url": "http://www.npr.org/sections/alltechconsidered/2016/11/17/495827410/from-hate-speech-to-fake-news-the-content-crisis-facing-mark-zuckerberg",
    "title": "From Hate Speech To Fake News: The Content Crisis Facing Mark Zuckerberg",
    "date": "2016-11-17",
    "authors": "Aarti Shahani",
    "content": "From Hate Speech To Fake News : The Content Crisis Facing Mark Zuckerberg\nMark Zuckerberg \u2014 one of the most insightful , adept leaders in the business world \u2014 has a problem . It 's a problem he has been slow to acknowledge , even though it 's become more apparent by the day .\nSeveral current and former Facebook employees tell NPR there is a lot of internal turmoil about how the platform does and does n't censor content that users find offensive . And outside Facebook , the public is regularly confounded by the company 's decisions \u2014 around controversial posts and around fake news .\n( Did Pope Francis really endorse Donald Trump ? Does Hillary Clinton really have a body double ? )\nBehind whatever the controversy of the moment happens to be , there 's a deep-seated problem . The problem is this : At age 19 , the then-boy genius started a social network that was basically a tech-savvy way to check out classmates in school . Then , over the course of 12 years , he made some very strategic decisions that have morphed Facebook into the most powerful distributor on Earth \u2014 the new front page of the news for more than 1 billion people every day . But Zuckerberg did n't sign up to head a media company \u2014 as in , one that has to make editorial judgments .\nHe and his team have made a very complex set of contradictory rules \u2014 a bias toward restricted speech for regular users , and toward free speech for `` news '' ( real or fake ) . And the company relies on a sprawling army of subcontractors to enforce the rules . People involved in trying to make it work say they 're in way over their heads . As one employee put it , `` We started out of a college dorm . I mean , c'mon , we 're Facebook . We never wanted to deal with this s * * * . ''\nNPR got the official version of how the company censors and leaves up content this summer when Facebook 's head of policy , Monika Bickert , agreed to a phone interview . We spoke with 10 current and former employees total , on the record and on background , for this investigation .\nIt 's hard to remember this sometimes , but Facebook has never claimed to be a free-speech platform . The company is trying to create a safe space where , unlike on Twitter , people can share without being trolled or shamed . Bickert is in charge of setting the content policies . The Community Standards , which are posted online , are the rules for everyday users .\nShe explained that when a user reports a piece of content that might be offensive , the company exercises its power to censor with precision .\n`` Context is so important . It 's critical when we are looking to determine whether or not something is hate speech , or a credible threat of violence , '' she said . `` We look at how a specific person shared a specific post or word or photo to Facebook . So we 're looking to see why did this particular share happen on Facebook ? Why did this particular post happen ? ''\nHowever , three of Bickert 's former colleagues tell a very different story of how Facebook deals with controversial content . They and others declined to be named for fear of job repercussions ( at Facebook or at their current employers , also Internet companies ) , but their descriptions are consistent with each other .\nWhen a user flags a post on Facebook \u2014 whether it 's a picture , video or text post \u2014 it goes to a little-known division called the `` community operations team . ''\nIn 2010 , the sources say , the team had a couple hundred workers in five countries . Facebook found it needed more hands on deck . After trying crowdsourcing solutions like CrowdFlower , the company turned to the consulting firm Accenture to put together a dedicated team of subcontractors . Sources say the team is now several thousand people , with some of the largest offices in Manila , the Philippines , and Warsaw , Poland .\nCurrent and former employees of Facebook say that they 've observed these subcontractors in action ; that they are told to go fast \u2014 very fast ; that they 're evaluated on speed ; and that on average , a worker makes a decision about a piece of flagged content once every 10 seconds .\nLet 's do a back-of-the-envelope calculation . Say a worker is doing an eight-hour shift , at the rate of one post per 10 seconds . That means they 're clearing 2,880 posts a day per person . When NPR ran these numbers by current and former employees , they said that sounds reasonable .\nA Facebook spokesperson says response times vary widely , depending on what is being reported ; that the vast majority of flagged content is not removed ; and that the numbers are off . Facebook did not provide alternative numbers .\nIf the sources \u2014 who have firsthand knowledge and spoke separately with NPR \u2014 are correct , then this may be the biggest editing \u2014 aka censorship \u2014 operation in the history of media . All the while , Facebook leaders insist they 're just running a `` platform , '' free of human judgment .\nA person who worked on this area of `` content management '' for Facebook ( as an employee , not a subcontractor ) says most of the content you see falls neatly into categories that do n't need deep reflection : `` That 's an erect penis . Check . '' So it 's not like the workers are analyzing every single one in detail .\nThe problem is , simple and complex items all go into the same big pile . So , the source says , `` you go on autopilot '' and do n't realize when `` you have to use judgment , in a system that does n't give you the time to make a real judgment . ''\nA classic case is something like `` Becky looks pregnant . '' It could be cyberbullying or a compliment . The subcontractor `` pretty much tosses a coin , '' the source says .\nHere 's another huge barrier . Because of privacy laws and technical glitches ( such as a post that is truncated to only show part of the conversation ) , the subcontractors typically do n't get to see the full context \u2014 to which Bickert referred so often .\nNPR decided to stress-test the system by flagging nearly 200 posts that could be considered hate speech \u2014 specifically , attacks against blacks and against whites in the U.S. We found that Facebook subcontractors were not consistent and made numerous mistakes , including in instances where a user calls for violence .\nWe say they were mistakes because the company changed its position in dozens of instances , removing some and restoring others \u2014 either when we flagged it a second time through the automated system or brought it to the attention of Facebook headquarters in Menlo Park , Calif .\nOne user shares a video of a police officer kicking someone on the ground , and another says they `` need to start organizing and sniping these bitches . '' This is a call to shoot cops .\nAnd it 's occurring in a very specific context : days after Philando Castile was shot and killed by an officer ( the shooting 's aftermath was live-streamed on Facebook ) ; and in a city in Minnesota that 's just a few miles away from where Castile lay bleeding .\nThe subcontractors did not remove the post . When NPR emailed Facebook headquarters about it , a spokesperson said they made a mistake and the post should have been removed .\nOne source tells NPR that the subcontractor `` likely '' could not see the full post if just the comment was flagged , could not see the profile of the user or even view the video to which the user was responding \u2014 again for privacy or technical reasons .\nDifferent projects within Facebook have to compete fiercely for engineering talent , and have to make the case that their to-do list is worth expensive company resources . Two sources say that over the years , it 's been hard to make that case for fixing the editing system .\nNPR shared many posts with the company to get specific feedback on why something stayed up or was taken down . This post , with the hashtag # blacklivesdontmatter , was left up . The spokesperson says it should have been removed , but the reviewer 's perspective is not the same as a regular Facebook user ; and because the company is protecting privacy , reviewers do n't have access to everything , which can affect their judgments . Also , the spokesperson says , reviewers do n't have the time a person at Facebook headquarters or at NPR may have to make a decision .\nAnother spokesperson notes that just because the reviewer has a limited view does not mean Bickert 's description is incorrect \u2014 that the two versions of the process are not mutually exclusive .\nThink of it this way . Every media outlet has its own culture and voice . If The New York Times is urban liberal , Fox News is conservative , and Playboy is racy , you could say Facebook aspires to be nice \u2014 a global brand that 's as easy to swallow as Coca-Cola . ( In this provocative talk at Harvard 's Shorenstein Center , law professor and author Jeffrey Rosen says Facebook favors `` civility '' over `` liberty . '' )\nNPR asked two newsroom veterans to put themselves in the position of a Facebook content arbiter . We do n't have access to Facebook 's internal guidance on hate speech , so they had to rely on their own judgment and familiarity with Facebook as users .\nLynette Clemetson , formerly with NPR and now at the University of Michigan , says of Post 1 , `` now that 's just dumb . '' Post 2 , she says , sounds like a `` rant . '' Chip Mahaney , a former managing editor at a Fox television station , agrees . Both experts venture to guess that the posts are acceptable speech on the platform .\nWhen NPR flagged the posts , Facebook censors decided to take them down . The spokesperson explains to NPR : It 's OK to use racial slurs when being self-referential . A black person can say things like `` my niggers . '' But no one can use a slur to attack an individual or group . That 's prohibited . A white person can not use the word `` nigger '' to mock or attack blacks . Blacks ca n't use `` crakkker '' ( in whatever spelling ) to offend whites .\nBut there are so many caveats and exceptions \u2014 particularly when it comes to interpreting images and videos .\nThis is a noose , the kind used to hang slaves . Beside it is a sign . While the letters are faded , you can make out the words : `` Nigger swing set . '' It was shared by a user named `` White Lives Matter 2 . ''\nThe news veterans would take it down . Clemetson says it 's not quite a call to violent action , but it 's certainly a reference to past action . `` It 's a reference to lynching \u2014 and making a joke of lynching . '' Mahaney says this is `` obviously a pretty difficult picture to look at '' and his instinct is , unless there 's a deeper context , `` I would say it has no place on here . ''\nThe spokesperson explains this historical reference does n't have a human victim clearly depicted . If the image included a person , a specific subject of a hate crime , then it would be removed .\nThe spokesperson also made a claim that has not panned out : that in some situations , like this one , Facebook requires the user who created the page to add their real name to the `` about '' section . By removing anonymity , the hope is , people will be more thoughtful about what they post .\nThe spokesperson says `` White Lives Matter 2 '' was told to identify him or herself promptly . Yet more than a month after NPR flagged the post , it had n't happened \u2014 yet another way Facebook 's enforcement mechanism is broken .\nIf the user rules were n't nuanced enough , add to that a new plot line : CEO Zuckerberg decided to forge partnerships with news media to make his social network the most powerful distributor of news on Earth . ( Facebook pays NPR and other news organizations to produce live videos for its site . )\nWhat Facebook has found in the process is that it 's much harder to censor high-profile newsmakers than it is to censor regular users . As one source says , `` Whoever screams the loudest gets our attention . We react . ''\nConsider the scandals around `` Napalm Girl '' and Donald Trump . In the first , Facebook was slammed for not allowing users to share a Pulitzer Prize-winning photo because it showed child nudity . In the second , Facebook came under criticism for allowing Donald Trump to call for a ban on Muslims coming to the U.S. \u2014 what is clearly hate speech under their regular rules , according to two former employees and a current one . One source said , `` he had n't even won the Republican primary yet . What we decided mattered . ''\nIn both cases , the company caved to public pressure and decided to bend the rules . The source says both decisions were `` highly controversial '' among employees ; and they signal that Facebook leadership is feeling pressure to move toward a free-speech standard for news distribution .\nHow to define news is a huge source of controversy \u2014 not just with the elections , but with every charged moment . After Trump won , some critics blamed fake news on Facebook for the election 's outcome . ( Zuckerberg dismisses that idea ) . After Philando Castile was shot by police , his video disappeared from Facebook . The company says it was `` a glitch '' and restored it . And the concern over its removal is part of an ongoing debate about what police-civilian standoffs should be live-streamed .\nSome in Silicon Valley dismiss the criticisms against Facebook as schadenfreude : Just like taxi drivers do n't like Uber , legacy media envies the success of the social platform and enjoys seeing its leadership on the hot seat .\nA former employee is not so dismissive and says there is a cultural problem , a stubborn blindness at Facebook and other leading Internet companies like Twitter . The source says : `` The hardest problems these companies face are n't technological . They are ethical , and there 's not as much rigor in how it 's done . ''\nAt a values level , some experts point out , Facebook has to decide if its solution is free speech ( the more people post , the more the truth rises ) , or clear restrictions .\nAnd technically , there 's no shortage of ideas about how to fix the process .\nA former employee says speech is so complex , you ca n't expect Facebook to arrive at the same decision each and every time ; but you can expect a company that consistently ranks among the 10 most valuable on Earth , by market cap , to put more thought and resources into its censorship machine .\nThe source argues Facebook could afford to make content management regional \u2014 have decisions come from the same country in which a post occurs .\nSpeech norms are highly regional . When Facebook first opened its offices in Hyderabad , India , a former employee says , the guidance the reviewers got was to remove sexual content . In a test run , they ended up removing French kissing . Senior management was blown away . The Indian reviewers were doing something Facebook did not expect but which makes perfect sense for local norms .\nHarvard business professor Ben Edelman says Facebook could invest engineering resources into categorizing the posts . `` It makes no sense at all , '' he says , that when a piece of content is flagged , it goes into one long line . The company could have the algorithm track what flagged content is getting the most circulation and move that up in the queue , he suggests .\nZuckerberg finds himself at the helm of a company that started as a tech company \u2014 run by algorithms , free of human judgment , the mythology went . And now he 's just so clearly the CEO of a media company \u2014 replete with highly complex rules ( What is hate speech anyway ? ) ; with double standards ( If it 's `` news '' it stays , if it 's a rant it goes ) ; and with an enforcement mechanism that is set up to fail .\nGabriela Mejias and Justina Vasquez contributed reporting to this story .",
    "content_original": "From Hate Speech To Fake News: The Content Crisis Facing Mark Zuckerberg\n\nEnlarge this image toggle caption Chelsea Beck/NPR Chelsea Beck/NPR\n\nEditor's Note: This story contains images and language that some readers may find disturbing.\n\nMark Zuckerberg \u2014 one of the most insightful, adept leaders in the business world \u2014 has a problem. It's a problem he has been slow to acknowledge, even though it's become more apparent by the day.\n\nSeveral current and former Facebook employees tell NPR there is a lot of internal turmoil about how the platform does and doesn't censor content that users find offensive. And outside Facebook, the public is regularly confounded by the company's decisions \u2014 around controversial posts and around fake news.\n\n(Did Pope Francis really endorse Donald Trump? Does Hillary Clinton really have a body double?)\n\nBehind whatever the controversy of the moment happens to be, there's a deep-seated problem. The problem is this: At age 19, the then-boy genius started a social network that was basically a tech-savvy way to check out classmates in school. Then, over the course of 12 years, he made some very strategic decisions that have morphed Facebook into the most powerful distributor on Earth \u2014 the new front page of the news for more than 1 billion people every day. But Zuckerberg didn't sign up to head a media company \u2014 as in, one that has to make editorial judgments.\n\nHe and his team have made a very complex set of contradictory rules \u2014 a bias toward restricted speech for regular users, and toward free speech for \"news\" (real or fake). And the company relies on a sprawling army of subcontractors to enforce the rules. People involved in trying to make it work say they're in way over their heads. As one employee put it, \"We started out of a college dorm. I mean, c'mon, we're Facebook. We never wanted to deal with this s***.\"\n\nSubcontractors running the show\n\nNPR got the official version of how the company censors and leaves up content this summer when Facebook's head of policy, Monika Bickert, agreed to a phone interview. We spoke with 10 current and former employees total, on the record and on background, for this investigation.\n\nIt's hard to remember this sometimes, but Facebook has never claimed to be a free-speech platform. The company is trying to create a safe space where, unlike on Twitter, people can share without being trolled or shamed. Bickert is in charge of setting the content policies. The Community Standards, which are posted online, are the rules for everyday users.\n\nShe explained that when a user reports a piece of content that might be offensive, the company exercises its power to censor with precision.\n\n\"Context is so important. It's critical when we are looking to determine whether or not something is hate speech, or a credible threat of violence,\" she said. \"We look at how a specific person shared a specific post or word or photo to Facebook. So we're looking to see why did this particular share happen on Facebook? Why did this particular post happen?\"\n\nHowever, three of Bickert's former colleagues tell a very different story of how Facebook deals with controversial content. They and others declined to be named for fear of job repercussions (at Facebook or at their current employers, also Internet companies), but their descriptions are consistent with each other.\n\nWhen a user flags a post on Facebook \u2014 whether it's a picture, video or text post \u2014 it goes to a little-known division called the \"community operations team.\"\n\nIn 2010, the sources say, the team had a couple hundred workers in five countries. Facebook found it needed more hands on deck. After trying crowdsourcing solutions like CrowdFlower, the company turned to the consulting firm Accenture to put together a dedicated team of subcontractors. Sources say the team is now several thousand people, with some of the largest offices in Manila, the Philippines, and Warsaw, Poland.\n\nCurrent and former employees of Facebook say that they've observed these subcontractors in action; that they are told to go fast \u2014 very fast; that they're evaluated on speed; and that on average, a worker makes a decision about a piece of flagged content once every 10 seconds.\n\nLet's do a back-of-the-envelope calculation. Say a worker is doing an eight-hour shift, at the rate of one post per 10 seconds. That means they're clearing 2,880 posts a day per person. When NPR ran these numbers by current and former employees, they said that sounds reasonable.\n\nA Facebook spokesperson says response times vary widely, depending on what is being reported; that the vast majority of flagged content is not removed; and that the numbers are off. Facebook did not provide alternative numbers.\n\nIf the sources \u2014 who have firsthand knowledge and spoke separately with NPR \u2014 are correct, then this may be the biggest editing \u2014 aka censorship \u2014 operation in the history of media. All the while, Facebook leaders insist they're just running a \"platform,\" free of human judgment.\n\nA person who worked on this area of \"content management\" for Facebook (as an employee, not a subcontractor) says most of the content you see falls neatly into categories that don't need deep reflection: \"That's an erect penis. Check.\" So it's not like the workers are analyzing every single one in detail.\n\nThe problem is, simple and complex items all go into the same big pile. So, the source says, \"you go on autopilot\" and don't realize when \"you have to use judgment, in a system that doesn't give you the time to make a real judgment.\"\n\nA classic case is something like \"Becky looks pregnant.\" It could be cyberbullying or a compliment. The subcontractor \"pretty much tosses a coin,\" the source says.\n\nHere's another huge barrier. Because of privacy laws and technical glitches (such as a post that is truncated to only show part of the conversation), the subcontractors typically don't get to see the full context \u2014 to which Bickert referred so often.\n\nFrequent errors\n\nThat could be the cause of frequent errors.\n\nNPR decided to stress-test the system by flagging nearly 200 posts that could be considered hate speech \u2014 specifically, attacks against blacks and against whites in the U.S. We found that Facebook subcontractors were not consistent and made numerous mistakes, including in instances where a user calls for violence.\n\nWe say they were mistakes because the company changed its position in dozens of instances, removing some and restoring others \u2014 either when we flagged it a second time through the automated system or brought it to the attention of Facebook headquarters in Menlo Park, Calif.\n\nConsider this post:\n\ntoggle caption Facebook/Screenshot by NPR\n\nOne user shares a video of a police officer kicking someone on the ground, and another says they \"need to start organizing and sniping these bitches.\" This is a call to shoot cops.\n\nAnd it's occurring in a very specific context: days after Philando Castile was shot and killed by an officer (the shooting's aftermath was live-streamed on Facebook); and in a city in Minnesota that's just a few miles away from where Castile lay bleeding.\n\nThe subcontractors did not remove the post. When NPR emailed Facebook headquarters about it, a spokesperson said they made a mistake and the post should have been removed.\n\nOne source tells NPR that the subcontractor \"likely\" could not see the full post if just the comment was flagged, could not see the profile of the user or even view the video to which the user was responding \u2014 again for privacy or technical reasons.\n\nDifferent projects within Facebook have to compete fiercely for engineering talent, and have to make the case that their to-do list is worth expensive company resources. Two sources say that over the years, it's been hard to make that case for fixing the editing system.\n\ntoggle caption Facebook/Screenshot by NPR\n\nNPR shared many posts with the company to get specific feedback on why something stayed up or was taken down. This post, with the hashtag #blacklivesdontmatter, was left up. The spokesperson says it should have been removed, but the reviewer's perspective is not the same as a regular Facebook user; and because the company is protecting privacy, reviewers don't have access to everything, which can affect their judgments. Also, the spokesperson says, reviewers don't have the time a person at Facebook headquarters or at NPR may have to make a decision.\n\nAnother spokesperson notes that just because the reviewer has a limited view does not mean Bickert's description is incorrect \u2014 that the two versions of the process are not mutually exclusive.\n\nA restrictive platform\n\nNPR also finds, interestingly, that Facebook can be strict.\n\nThink of it this way. Every media outlet has its own culture and voice. If The New York Times is urban liberal, Fox News is conservative, and Playboy is racy, you could say Facebook aspires to be nice \u2014 a global brand that's as easy to swallow as Coca-Cola. (In this provocative talk at Harvard's Shorenstein Center, law professor and author Jeffrey Rosen says Facebook favors \"civility\" over \"liberty.\")\n\nThe bias is evident in these real-life examples:\n\ntoggle caption Facebook/Screenshot by NPR\n\ntoggle caption Facebook/Screenshot by NPR\n\nNPR asked two newsroom veterans to put themselves in the position of a Facebook content arbiter. We don't have access to Facebook's internal guidance on hate speech, so they had to rely on their own judgment and familiarity with Facebook as users.\n\nLynette Clemetson, formerly with NPR and now at the University of Michigan, says of Post 1, \"now that's just dumb.\" Post 2, she says, sounds like a \"rant.\" Chip Mahaney, a former managing editor at a Fox television station, agrees. Both experts venture to guess that the posts are acceptable speech on the platform.\n\nTo their surprise, they're not.\n\nWhen NPR flagged the posts, Facebook censors decided to take them down. The spokesperson explains to NPR: It's OK to use racial slurs when being self-referential. A black person can say things like \"my niggers.\" But no one can use a slur to attack an individual or group. That's prohibited. A white person cannot use the word \"nigger\" to mock or attack blacks. Blacks can't use \"crakkker\" (in whatever spelling) to offend whites.\n\nWiggle room in pictures\n\nBut there are so many caveats and exceptions \u2014 particularly when it comes to interpreting images and videos.\n\nConsider this post:\n\ntoggle caption Facebook/Screenshot by NPR\n\nThis is a noose, the kind used to hang slaves. Beside it is a sign. While the letters are faded, you can make out the words: \"Nigger swing set.\" It was shared by a user named \"White Lives Matter 2.\"\n\nThe news veterans would take it down. Clemetson says it's not quite a call to violent action, but it's certainly a reference to past action. \"It's a reference to lynching \u2014 and making a joke of lynching.\" Mahaney says this is \"obviously a pretty difficult picture to look at\" and his instinct is, unless there's a deeper context, \"I would say it has no place on here.\"\n\nFacebook left it up, and stands by that decision.\n\nThe spokesperson explains this historical reference doesn't have a human victim clearly depicted. If the image included a person, a specific subject of a hate crime, then it would be removed.\n\nThe spokesperson also made a claim that has not panned out: that in some situations, like this one, Facebook requires the user who created the page to add their real name to the \"about\" section. By removing anonymity, the hope is, people will be more thoughtful about what they post.\n\nThe spokesperson says \"White Lives Matter 2\" was told to identify him or herself promptly. Yet more than a month after NPR flagged the post, it hadn't happened \u2014 yet another way Facebook's enforcement mechanism is broken.\n\nWith news, a double standard emerges\n\nIf the user rules weren't nuanced enough, add to that a new plot line: CEO Zuckerberg decided to forge partnerships with news media to make his social network the most powerful distributor of news on Earth. (Facebook pays NPR and other news organizations to produce live videos for its site.)\n\nWhat Facebook has found in the process is that it's much harder to censor high-profile newsmakers than it is to censor regular users. As one source says, \"Whoever screams the loudest gets our attention. We react.\"\n\nAnd that's making newsworthy content a whole other category.\n\nConsider the scandals around \"Napalm Girl\" and Donald Trump. In the first, Facebook was slammed for not allowing users to share a Pulitzer Prize-winning photo because it showed child nudity. In the second, Facebook came under criticism for allowing Donald Trump to call for a ban on Muslims coming to the U.S. \u2014 what is clearly hate speech under their regular rules, according to two former employees and a current one. One source said, \"he hadn't even won the Republican primary yet. What we decided mattered.\"\n\nIn both cases, the company caved to public pressure and decided to bend the rules. The source says both decisions were \"highly controversial\" among employees; and they signal that Facebook leadership is feeling pressure to move toward a free-speech standard for news distribution.\n\nHow to define news is a huge source of controversy \u2014 not just with the elections, but with every charged moment. After Trump won, some critics blamed fake news on Facebook for the election's outcome. (Zuckerberg dismisses that idea). After Philando Castile was shot by police, his video disappeared from Facebook. The company says it was \"a glitch\" and restored it. And the concern over its removal is part of an ongoing debate about what police-civilian standoffs should be live-streamed.\n\nWhat now?\n\nSome in Silicon Valley dismiss the criticisms against Facebook as schadenfreude: Just like taxi drivers don't like Uber, legacy media envies the success of the social platform and enjoys seeing its leadership on the hot seat.\n\nA former employee is not so dismissive and says there is a cultural problem, a stubborn blindness at Facebook and other leading Internet companies like Twitter. The source says: \"The hardest problems these companies face aren't technological. They are ethical, and there's not as much rigor in how it's done.\"\n\nAt a values level, some experts point out, Facebook has to decide if its solution is free speech (the more people post, the more the truth rises), or clear restrictions.\n\nAnd technically, there's no shortage of ideas about how to fix the process.\n\nA former employee says speech is so complex, you can't expect Facebook to arrive at the same decision each and every time; but you can expect a company that consistently ranks among the 10 most valuable on Earth, by market cap, to put more thought and resources into its censorship machine.\n\nThe source argues Facebook could afford to make content management regional \u2014 have decisions come from the same country in which a post occurs.\n\nSpeech norms are highly regional. When Facebook first opened its offices in Hyderabad, India, a former employee says, the guidance the reviewers got was to remove sexual content. In a test run, they ended up removing French kissing. Senior management was blown away. The Indian reviewers were doing something Facebook did not expect but which makes perfect sense for local norms.\n\nHarvard business professor Ben Edelman says Facebook could invest engineering resources into categorizing the posts. \"It makes no sense at all,\" he says, that when a piece of content is flagged, it goes into one long line. The company could have the algorithm track what flagged content is getting the most circulation and move that up in the queue, he suggests.\n\nZuckerberg finds himself at the helm of a company that started as a tech company \u2014 run by algorithms, free of human judgment, the mythology went. And now he's just so clearly the CEO of a media company \u2014 replete with highly complex rules (What is hate speech anyway?); with double standards (If it's \"news\" it stays, if it's a rant it goes); and with an enforcement mechanism that is set up to fail.\n\nGabriela Mejias and Justina Vasquez contributed reporting to this story.",
    "source_url": "www.npr.org",
    "bias_text": "center",
    "ID": "v0zeGgMJMIurhhmD"
}