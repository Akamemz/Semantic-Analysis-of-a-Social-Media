{
    "topic": "technology",
    "source": "Associated Press",
    "bias": 1,
    "url": "https://www.apnews.com/f97c24dab4f34bd0b48b36f2988952a4",
    "title": "Facebook auto-generates videos celebrating extremist images",
    "date": "2019-05-09",
    "authors": "Desmond Butler, Barbara Ortutay",
    "content": "A banner reading `` The Islamic State '' is displayed on the Facebook page of a user identifying himself as Nawan Al-Farancsa . The page was still live Tuesday , May 7 , 2019 , when the screen grab was made . Facebook says it has robust systems in place to remove content from extremist groups , but a sealed whistleblower 's complaint reviewed by the AP says banned content remains on the web and is easy to find . ( Facebook via AP )\nA banner reading `` The Islamic State '' is displayed on the Facebook page of a user identifying himself as Nawan Al-Farancsa . The page was still live Tuesday , May 7 , 2019 , when the screen grab was made . Facebook says it has robust systems in place to remove content from extremist groups , but a sealed whistleblower 's complaint reviewed by the AP says banned content remains on the web and is easy to find . ( Facebook via AP )\nWASHINGTON ( AP ) \u2014 The animated video begins with a photo of the black flags of jihad . Seconds later , it flashes highlights of a year of social media posts : plaques of anti-Semitic verses , talk of retribution and a photo of two men carrying more jihadi flags while they burn the stars and stripes .\nIt wasn \u2019 t produced by extremists ; it was created by Facebook . In a clever bit of self-promotion , the social media giant takes a year of a user \u2019 s content and auto-generates a celebratory video . In this case , the user called himself \u201c Abdel-Rahim Moussa , the Caliphate . \u201d\n\u201c Thanks for being here , from Facebook , \u201d the video concludes in a cartoon bubble before flashing the company \u2019 s famous \u201c thumbs up . \u201d\nFacebook likes to give the impression it \u2019 s staying ahead of extremists by taking down their posts , often before users even see them . But a confidential whistleblower \u2019 s complaint to the Securities and Exchange Commission obtained by The \u2588\u2588\u2588 alleges the social media company has exaggerated its success . Even worse , it shows that the company is inadvertently making use of propaganda by militant groups to auto-generate videos and pages that could be used for networking by extremists .\nAccording to the complaint , over a five-month period last year , researchers monitored pages by users who affiliated themselves with groups the U.S. State Department has designated as terrorist organizations . In that period , 38 % of the posts with prominent symbols of extremist groups were removed . In its own review , the AP found that as of this month , much of the banned content cited in the study \u2014 an execution video , images of severed heads , propaganda honoring martyred militants \u2014 slipped through the algorithmic web and remained easy to find on Facebook .\nThe complaint is landing as Facebook tries to stay ahead of a growing array of criticism over its privacy practices and its ability to keep hate speech , live-streamed murders and suicides off its service . In the face of criticism , CEO Mark Zuckerberg has spoken of his pride in the company \u2019 s ability to weed out violent posts automatically through artificial intelligence . During an earnings call last month , for instance , he repeated a carefully worded formulation that Facebook has been employing .\n\u201c In areas like terrorism , for al-Qaida and ISIS-related content , now 99 percent of the content that we take down in the category our systems flag proactively before anyone sees it , \u201d he said . Then he added : \u201c That \u2019 s what really good looks like . \u201d\nZuckerberg did not offer an estimate of how much of total prohibited material is being removed .\nThe research behind the SEC complaint is aimed at spotlighting glaring flaws in the company \u2019 s approach . Last year , researchers began monitoring users who explicitly identified themselves as members of extremist groups . It wasn \u2019 t hard to document . Some of these people even list the extremist groups as their employers . One profile heralded by the black flag of an al-Qaida affiliated group listed his employer , perhaps facetiously , as Facebook . The profile that included the auto-generated video with the flag burning also had a video of al-Qaida leader Ayman al-Zawahiri urging jihadi groups not to fight among themselves .\nWhile the study is far from comprehensive \u2014 in part because Facebook rarely makes much of its data publicly available \u2014 researchers involved in the project say the ease of identifying these profiles using a basic keyword search and the fact that so few of them have been removed suggest that Facebook \u2019 s claims that its systems catch most extremist content are not accurate .\n\u201c I mean , that \u2019 s just stretching the imagination to beyond incredulity , \u201d says Amr Al Azm , one of the researchers involved in the project . \u201c If a small group of researchers can find hundreds of pages of content by simple searches , why can \u2019 t a giant company with all its resources do it ? \u201d\nAl Azm , a professor of history and anthropology at Shawnee State University in Ohio , has also directed a group in Syria documenting the looting and smuggling of antiquities .\nFacebook concedes that its systems are not perfect , but says it \u2019 s making improvements .\n\u201c After making heavy investments , we are detecting and removing terrorism content at a far higher success rate than even two years ago , \u201d the company said in a statement . \u201c We don \u2019 t claim to find everything and we remain vigilant in our efforts against terrorist groups around the world . \u201d\nReacting to the AP \u2019 s reporting , Rep. Bennie Thompson , D-Miss. , the chairman of the House Homeland Security Committee expressed frustration that Facebook has made so little progress on blocking content despite reassurances he received from the company .\n\u201c This is yet another deeply worrisome example of Facebook \u2019 s inability to manage its own platforms \u2014 and the extent to which it needs to clean up its act , \u201d he said . \u201c Facebook must not only rid its platforms of terrorist and extremist content , but it also needs to be able to prevent it from being amplified . \u201d\nBut as a stark indication of how easily users can evade Facebook , one page from a user called \u201c Nawan al-Farancsa \u201d has a header whose white lettering against a black background says in English \u201c The Islamic State. \u201d The banner is punctuated with a photo of an explosive mushroom cloud rising from a city .\nThe profile should have caught the attention of Facebook \u2014 as well as counter-intelligence agencies . It was created in June 2018 , lists the user as coming from Chechnya , once a militant hotspot . It says he lived in Heidelberg , Germany , and studied at a university in Indonesia . Some of the user \u2019 s friends also posted militant content .\nThe page , still up in recent days , apparently escaped Facebook \u2019 s systems , because of an obvious and long-running evasion of moderation that Facebook should be adept at recognizing : The letters were not searchable text but embedded in a graphic block . But the company says its technology scans audio , video and text \u2014 including when it is embedded \u2014 for images that reflect violence , weapons or logos of prohibited groups .\nThe social networking giant has endured a rough two years beginning in 2016 , when Russia \u2019 s use of social media to meddle with the U.S. presidential elections came into focus . Zuckerberg initially downplayed the role Facebook played in the influence operation by Russian intelligence , but the company later apologized .\nFacebook says it now employs 30,000 people who work on its safety and security practices , reviewing potentially harmful material and anything else that might not belong on the site . Still , the company is putting a lot of its faith in artificial intelligence and its systems \u2019 ability to eventually weed out bad stuff without the help of humans . The new research suggests that goal is a long way away and some critics allege that the company is not making a sincere effort .\nWhen the material isn \u2019 t removed , it \u2019 s treated the same as anything else posted by Facebook \u2019 s 2.4 billion users \u2014 celebrated in animated videos , linked and categorized and recommended by algorithms .\nBut it \u2019 s not just the algorithms that are to blame . The researchers found that some extremists are using Facebook \u2019 s \u201c Frame Studio \u201d to post militant propaganda . The tool lets people decorate their profile photos within graphic frames \u2014 to support causes or celebrate birthdays , for instance . Facebook says that those framed images must be approved by the company before they are posted .\nHany Farid , a digital forensics expert at the University of California , Berkeley , who advises the Counter-Extremism Project , a New York and London-based group focused on combatting extremist messaging , says that Facebook \u2019 s artificial intelligence system is failing . He says the company is not motivated to tackle the problem because it would be expensive .\n\u201c The whole infrastructure is fundamentally flawed , \u201d he said . \u201c And there \u2019 s very little appetite to fix it because what Facebook and the other social media companies know is that once they start being responsible for material on their platforms it opens up a whole can of worms . \u201d\nAnother Facebook auto-generation function gone awry scrapes employment information from user \u2019 s pages to create business pages . The function is supposed to produce pages meant to help companies network , but in many cases they are serving as a branded landing space for extremist groups . The function allows Facebook users to like pages for extremist organizations , including al-Qaida , the Islamic State group and the Somali-based al-Shabab , effectively providing a list of sympathizers for recruiters .\nAt the top of an auto-generated page for al-Qaida in the Arabian Peninsula , the AP found a photo of the damaged hull of the USS Cole , which was bombed by al-Qaida in a 2000 attack off the coast of Yemen that killed 17 U.S. Navy sailors . It \u2019 s the defining image in AQAP \u2019 s own propaganda . The page includes the Wikipedia entry for the group and had been liked by 277 people when last viewed this week .\nAs part of the investigation for the complaint , Al Azm \u2019 s researchers in Syria looked closely at the profiles of 63 accounts that liked the auto-generated page for Hay \u2019 at Tahrir al-Sham , a group that merged from militant groups in Syria , including the al-Qaida affiliated al-Nusra Front . The researchers were able to confirm that 31 of the profiles matched real people in Syria . Some of them turned out to be the same individuals Al Azm \u2019 s team was monitoring in a separate project to document the financing of militant groups through antiquities smuggling .\nFacebook also faces a challenge with U.S. hate groups . In March , the company announced that it was expanding its prohibited content to also include white nationalist and white separatist content\u2014 previously it only took action with white supremacist content . It says that it has banned more than 200 white supremacist groups . But it \u2019 s still easy to find symbols of supremacy and racial hatred .\nThe researchers in the SEC complaint identified over 30 auto-generated pages for white supremacist groups , whose content Facebook prohibits . They include \u201c The American Nazi Party \u201d and the \u201c New Aryan Empire. \u201d A page created for the \u201c Aryan Brotherhood Headquarters \u201d marks the office on a map and asks whether users recommend it . One endorser posted a question : \u201c How can a brother get in the house . \u201d\nEven supremacists flagged by law enforcement are slipping through the net . Following a sweep of arrests beginning in October , federal prosecutors in Arkansas indicted dozens of members of a drug trafficking ring linked to the New Aryan Empire . A legal document from February paints a brutal picture of the group , alleging murder , kidnapping and intimidation of witnesses that in one instance involved using a searing-hot knife to scar someone \u2019 s face . It also alleges the group used Facebook to discuss New Aryan Empire business .\nBut many of the individuals named in the indictment have Facebook pages that were still up in recent days . They leave no doubt of the users \u2019 white supremacist affiliation , posting images of Hitler , swastikas and a numerical symbol of the New Aryan Empire slogan , \u201c To The Dirt \u201d \u2014 the members \u2019 pledge to remain loyal to the end . One of the group \u2019 s indicted leaders , Jeffrey Knox , listed his job as \u201c stomp down Honky. \u201d Facebook then auto-generated a \u201c stomp down Honky \u201d business page .\nSocial media companies have broad protection in U.S. law from liability stemming from the content that users post on their sites . But Facebook \u2019 s role in generating videos and pages from extremist content raises questions about exposure . Legal analysts contacted by the AP differed on whether the discovery could open the company up to lawsuits .\nAt a minimum , the research behind the SEC complaint illustrates the company \u2019 s limited approach to combatting online extremism . The U.S. State Department lists dozens of groups as \u201c designated foreign terrorist organizations \u201d but Facebook in its public statements says it focuses its efforts on two , the Islamic State group and al-Qaida . But even with those two targets , Facebook \u2019 s algorithms often miss the names of affiliated groups . Al Azm says Facebook \u2019 s method seems to be less effective with Arabic script .\nFor instance , a search in Arabic for \u201c Al-Qaida in the Arabian Peninsula \u201d turns up not only posts , but an auto-generated business page . One user listed his occupation as \u201c Former Sniper \u201d at \u201c Al-Qaida in the Arabian Peninsula \u201d written in Arabic . Another user evaded Facebook \u2019 s cull by reversing the order of the countries in the Arabic for ISIS or \u201c Islamic State of Iraq and Syria . \u201d\nJohn Kostyack , a lawyer with the National Whistleblower Center in Washington who represents the anonymous plaintiff behind the complaint , said the goal is to make Facebook take a more robust approach to counteracting extremist propaganda .\n\u201c Right now we \u2019 re hearing stories of what happened in New Zealand and Sri Lanka \u2014 just heartbreaking massacres where the groups that came forward were clearly openly recruiting and networking on Facebook and other social media , \u201d he said . \u201c That \u2019 s not going to stop unless we develop a public policy to deal with it , unless we create some kind of sense of corporate social responsibility . \u201d\nFarid , the digital forensics expert , says that Facebook built its infrastructure without thinking through the dangers stemming from content and is now trying to retrofit solutions .\n\u201c The policy of this platform has been : \u2018 Move fast and break things. \u2019 I actually think that for once their motto was actually accurate , \u201d he says . \u201c The strategy was grow , grow , grow , profit , profit , profit and then go back and try to deal with whatever problems there are . \u201d\nFollow the authors on Twitter at https : //twitter.com/desmondbutler and https : //twitter.com/BarbaraOrtutay .\nHave a tip ? Contact the authors securely at https : //www.ap.org/tips .",
    "content_original": "A banner reading \"The Islamic State\" is displayed on the Facebook page of a user identifying himself as Nawan Al-Farancsa. The page was still live Tuesday, May 7, 2019, when the screen grab was made. Facebook says it has robust systems in place to remove content from extremist groups, but a sealed whistleblower's complaint reviewed by the AP says banned content remains on the web and is easy to find. (Facebook via AP)\n\nA banner reading \"The Islamic State\" is displayed on the Facebook page of a user identifying himself as Nawan Al-Farancsa. The page was still live Tuesday, May 7, 2019, when the screen grab was made. Facebook says it has robust systems in place to remove content from extremist groups, but a sealed whistleblower's complaint reviewed by the AP says banned content remains on the web and is easy to find. (Facebook via AP)\n\nWASHINGTON (AP) \u2014 The animated video begins with a photo of the black flags of jihad. Seconds later, it flashes highlights of a year of social media posts: plaques of anti-Semitic verses, talk of retribution and a photo of two men carrying more jihadi flags while they burn the stars and stripes.\n\nIt wasn\u2019t produced by extremists; it was created by Facebook. In a clever bit of self-promotion, the social media giant takes a year of a user\u2019s content and auto-generates a celebratory video. In this case, the user called himself \u201cAbdel-Rahim Moussa, the Caliphate.\u201d\n\n\u201cThanks for being here, from Facebook,\u201d the video concludes in a cartoon bubble before flashing the company\u2019s famous \u201cthumbs up.\u201d\n\nFacebook likes to give the impression it\u2019s staying ahead of extremists by taking down their posts, often before users even see them. But a confidential whistleblower\u2019s complaint to the Securities and Exchange Commission obtained by The Associated Press alleges the social media company has exaggerated its success. Even worse, it shows that the company is inadvertently making use of propaganda by militant groups to auto-generate videos and pages that could be used for networking by extremists.\n\nAccording to the complaint, over a five-month period last year, researchers monitored pages by users who affiliated themselves with groups the U.S. State Department has designated as terrorist organizations. In that period, 38% of the posts with prominent symbols of extremist groups were removed. In its own review, the AP found that as of this month, much of the banned content cited in the study \u2014 an execution video, images of severed heads, propaganda honoring martyred militants \u2014 slipped through the algorithmic web and remained easy to find on Facebook.\n\nThe complaint is landing as Facebook tries to stay ahead of a growing array of criticism over its privacy practices and its ability to keep hate speech, live-streamed murders and suicides off its service. In the face of criticism, CEO Mark Zuckerberg has spoken of his pride in the company\u2019s ability to weed out violent posts automatically through artificial intelligence. During an earnings call last month, for instance, he repeated a carefully worded formulation that Facebook has been employing.\n\n\u201cIn areas like terrorism, for al-Qaida and ISIS-related content, now 99 percent of the content that we take down in the category our systems flag proactively before anyone sees it,\u201d he said. Then he added: \u201cThat\u2019s what really good looks like.\u201d\n\nZuckerberg did not offer an estimate of how much of total prohibited material is being removed.\n\nThe research behind the SEC complaint is aimed at spotlighting glaring flaws in the company\u2019s approach. Last year, researchers began monitoring users who explicitly identified themselves as members of extremist groups. It wasn\u2019t hard to document. Some of these people even list the extremist groups as their employers. One profile heralded by the black flag of an al-Qaida affiliated group listed his employer, perhaps facetiously, as Facebook. The profile that included the auto-generated video with the flag burning also had a video of al-Qaida leader Ayman al-Zawahiri urging jihadi groups not to fight among themselves.\n\nWhile the study is far from comprehensive \u2014 in part because Facebook rarely makes much of its data publicly available \u2014 researchers involved in the project say the ease of identifying these profiles using a basic keyword search and the fact that so few of them have been removed suggest that Facebook\u2019s claims that its systems catch most extremist content are not accurate.\n\n\u201cI mean, that\u2019s just stretching the imagination to beyond incredulity,\u201d says Amr Al Azm, one of the researchers involved in the project. \u201cIf a small group of researchers can find hundreds of pages of content by simple searches, why can\u2019t a giant company with all its resources do it?\u201d\n\nAl Azm, a professor of history and anthropology at Shawnee State University in Ohio, has also directed a group in Syria documenting the looting and smuggling of antiquities.\n\nFacebook concedes that its systems are not perfect, but says it\u2019s making improvements.\n\n\u201cAfter making heavy investments, we are detecting and removing terrorism content at a far higher success rate than even two years ago,\u201d the company said in a statement. \u201cWe don\u2019t claim to find everything and we remain vigilant in our efforts against terrorist groups around the world.\u201d\n\nReacting to the AP\u2019s reporting, Rep. Bennie Thompson, D-Miss., the chairman of the House Homeland Security Committee expressed frustration that Facebook has made so little progress on blocking content despite reassurances he received from the company.\n\n\u201cThis is yet another deeply worrisome example of Facebook\u2019s inability to manage its own platforms \u2014 and the extent to which it needs to clean up its act,\u201d he said. \u201cFacebook must not only rid its platforms of terrorist and extremist content, but it also needs to be able to prevent it from being amplified.\u201d\n\nBut as a stark indication of how easily users can evade Facebook, one page from a user called \u201cNawan al-Farancsa\u201d has a header whose white lettering against a black background says in English \u201cThe Islamic State.\u201d The banner is punctuated with a photo of an explosive mushroom cloud rising from a city.\n\nThe profile should have caught the attention of Facebook \u2014 as well as counter-intelligence agencies. It was created in June 2018, lists the user as coming from Chechnya, once a militant hotspot. It says he lived in Heidelberg, Germany, and studied at a university in Indonesia. Some of the user\u2019s friends also posted militant content.\n\nThe page, still up in recent days, apparently escaped Facebook\u2019s systems, because of an obvious and long-running evasion of moderation that Facebook should be adept at recognizing: The letters were not searchable text but embedded in a graphic block. But the company says its technology scans audio, video and text \u2014 including when it is embedded \u2014 for images that reflect violence, weapons or logos of prohibited groups.\n\nThe social networking giant has endured a rough two years beginning in 2016, when Russia\u2019s use of social media to meddle with the U.S. presidential elections came into focus. Zuckerberg initially downplayed the role Facebook played in the influence operation by Russian intelligence, but the company later apologized.\n\nFacebook says it now employs 30,000 people who work on its safety and security practices, reviewing potentially harmful material and anything else that might not belong on the site. Still, the company is putting a lot of its faith in artificial intelligence and its systems\u2019 ability to eventually weed out bad stuff without the help of humans. The new research suggests that goal is a long way away and some critics allege that the company is not making a sincere effort.\n\nWhen the material isn\u2019t removed, it\u2019s treated the same as anything else posted by Facebook\u2019s 2.4 billion users \u2014 celebrated in animated videos, linked and categorized and recommended by algorithms.\n\nBut it\u2019s not just the algorithms that are to blame. The researchers found that some extremists are using Facebook\u2019s \u201cFrame Studio\u201d to post militant propaganda. The tool lets people decorate their profile photos within graphic frames \u2014 to support causes or celebrate birthdays, for instance. Facebook says that those framed images must be approved by the company before they are posted.\n\nHany Farid, a digital forensics expert at the University of California, Berkeley, who advises the Counter-Extremism Project, a New York and London-based group focused on combatting extremist messaging, says that Facebook\u2019s artificial intelligence system is failing. He says the company is not motivated to tackle the problem because it would be expensive.\n\n\u201cThe whole infrastructure is fundamentally flawed,\u201d he said. \u201cAnd there\u2019s very little appetite to fix it because what Facebook and the other social media companies know is that once they start being responsible for material on their platforms it opens up a whole can of worms.\u201d\n\nAnother Facebook auto-generation function gone awry scrapes employment information from user\u2019s pages to create business pages. The function is supposed to produce pages meant to help companies network, but in many cases they are serving as a branded landing space for extremist groups. The function allows Facebook users to like pages for extremist organizations, including al-Qaida, the Islamic State group and the Somali-based al-Shabab, effectively providing a list of sympathizers for recruiters.\n\nAt the top of an auto-generated page for al-Qaida in the Arabian Peninsula, the AP found a photo of the damaged hull of the USS Cole, which was bombed by al-Qaida in a 2000 attack off the coast of Yemen that killed 17 U.S. Navy sailors. It\u2019s the defining image in AQAP\u2019s own propaganda. The page includes the Wikipedia entry for the group and had been liked by 277 people when last viewed this week.\n\nAs part of the investigation for the complaint, Al Azm\u2019s researchers in Syria looked closely at the profiles of 63 accounts that liked the auto-generated page for Hay\u2019at Tahrir al-Sham, a group that merged from militant groups in Syria, including the al-Qaida affiliated al-Nusra Front. The researchers were able to confirm that 31 of the profiles matched real people in Syria. Some of them turned out to be the same individuals Al Azm\u2019s team was monitoring in a separate project to document the financing of militant groups through antiquities smuggling.\n\nFacebook also faces a challenge with U.S. hate groups. In March, the company announced that it was expanding its prohibited content to also include white nationalist and white separatist content\u2014 previously it only took action with white supremacist content. It says that it has banned more than 200 white supremacist groups. But it\u2019s still easy to find symbols of supremacy and racial hatred.\n\nThe researchers in the SEC complaint identified over 30 auto-generated pages for white supremacist groups, whose content Facebook prohibits. They include \u201cThe American Nazi Party\u201d and the \u201cNew Aryan Empire.\u201d A page created for the \u201cAryan Brotherhood Headquarters\u201d marks the office on a map and asks whether users recommend it. One endorser posted a question: \u201cHow can a brother get in the house.\u201d\n\nEven supremacists flagged by law enforcement are slipping through the net. Following a sweep of arrests beginning in October, federal prosecutors in Arkansas indicted dozens of members of a drug trafficking ring linked to the New Aryan Empire. A legal document from February paints a brutal picture of the group, alleging murder, kidnapping and intimidation of witnesses that in one instance involved using a searing-hot knife to scar someone\u2019s face. It also alleges the group used Facebook to discuss New Aryan Empire business.\n\nBut many of the individuals named in the indictment have Facebook pages that were still up in recent days. They leave no doubt of the users\u2019 white supremacist affiliation, posting images of Hitler, swastikas and a numerical symbol of the New Aryan Empire slogan, \u201cTo The Dirt\u201d \u2014 the members\u2019 pledge to remain loyal to the end. One of the group\u2019s indicted leaders, Jeffrey Knox, listed his job as \u201cstomp down Honky.\u201d Facebook then auto-generated a \u201cstomp down Honky\u201d business page.\n\nSocial media companies have broad protection in U.S. law from liability stemming from the content that users post on their sites. But Facebook\u2019s role in generating videos and pages from extremist content raises questions about exposure. Legal analysts contacted by the AP differed on whether the discovery could open the company up to lawsuits.\n\nAt a minimum, the research behind the SEC complaint illustrates the company\u2019s limited approach to combatting online extremism. The U.S. State Department lists dozens of groups as \u201cdesignated foreign terrorist organizations\u201d but Facebook in its public statements says it focuses its efforts on two, the Islamic State group and al-Qaida. But even with those two targets, Facebook\u2019s algorithms often miss the names of affiliated groups. Al Azm says Facebook\u2019s method seems to be less effective with Arabic script.\n\nFor instance, a search in Arabic for \u201cAl-Qaida in the Arabian Peninsula\u201d turns up not only posts, but an auto-generated business page. One user listed his occupation as \u201cFormer Sniper\u201d at \u201cAl-Qaida in the Arabian Peninsula\u201d written in Arabic. Another user evaded Facebook\u2019s cull by reversing the order of the countries in the Arabic for ISIS or \u201cIslamic State of Iraq and Syria.\u201d\n\nJohn Kostyack, a lawyer with the National Whistleblower Center in Washington who represents the anonymous plaintiff behind the complaint, said the goal is to make Facebook take a more robust approach to counteracting extremist propaganda.\n\n\u201cRight now we\u2019re hearing stories of what happened in New Zealand and Sri Lanka \u2014 just heartbreaking massacres where the groups that came forward were clearly openly recruiting and networking on Facebook and other social media,\u201d he said. \u201cThat\u2019s not going to stop unless we develop a public policy to deal with it, unless we create some kind of sense of corporate social responsibility.\u201d\n\nFarid, the digital forensics expert, says that Facebook built its infrastructure without thinking through the dangers stemming from content and is now trying to retrofit solutions.\n\n\u201cThe policy of this platform has been: \u2018Move fast and break things.\u2019 I actually think that for once their motto was actually accurate,\u201d he says. \u201cThe strategy was grow, grow, grow, profit, profit, profit and then go back and try to deal with whatever problems there are.\u201d\n\n___\n\nBarbara Ortutay reported from San Francisco. Associated Press writer Maggie Michael contributed to this report.\n\n___\n\nFollow the authors on Twitter at https://twitter.com/desmondbutler and https://twitter.com/BarbaraOrtutay .\n\n__\n\nHave a tip? Contact the authors securely at https://www.ap.org/tips .",
    "source_url": "www.apnews.com",
    "bias_text": "center",
    "ID": "YhkYI4wiT8DT7peb"
}