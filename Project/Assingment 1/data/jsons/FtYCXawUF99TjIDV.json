{
    "topic": "fake_news",
    "source": "Guest Writer - Left",
    "bias": 0,
    "url": "https://www.nytimes.com/2018/10/15/opinion/facebook-fake-news-philosophy.html?action=click&module=Opinion&pgtype=Homepage",
    "title": "OPINION: How to Fix Fake News",
    "date": "2018-10-15",
    "authors": "Regina Rini",
    "content": "Here \u2019 s a system that might help , and it is based on something that Facebook already does to prevent the spread of fake news . Currently , Facebook asks independent fact-checking organizations from across the political spectrum to identify false and misleading information . Whenever users try to post something that has been identified as fake news , they are confronted by a pop-up that explains the problems with the news and asks them to confirm if they \u2019 d like to continue . None of these users are prevented from posting stories whose facts are in dispute , but they are required to know that what they are sharing may be false or misleading .\nFacebook has been openly using this system since December 2016 . Less openly , they have also been keeping tabs on how often its users attempt to flag stories as fake news , and , using this feature , they have been calculating the epistemic reliability of their users . The Washington Post reported in August that Facebook secretly calculates scores that represent how often users \u2019 flags align with the analysis of independent fact-checkers . Facebook only uses this data internally , to identify abuse of the flagging system , and does not release it to users . I can \u2019 t find out my own reputation score , or the scores of any of my friends .\nThis system and the secrecy around it may come across as a bit creepy \u2014 and the public trust in Facebook has been seriously and justifiably damaged \u2014 but I think that Facebook is on to something . Last year , in a paper published in the Kennedy Institute of Ethics Journal , I proposed a somewhat different system . The key difference between my system and the one that Facebook has implemented is transparency : Facebook should track and display how often each user decides to share disputed information after being warned that the information might be false or misleading .\nInstead of using this data to calculate a secret score , Facebook should display a simple reliability marker on every post and comment . Imagine a little colored dot next to the user \u2019 s name , similar to the blue verification badges Facebook and Twitter give to trusted accounts : a green dot could indicate that the user hasn \u2019 t chosen to share much disputed news , a yellow dot could indicate that they do it sometimes , and a red dot could indicate that they do it often . These reliability markers would allow anyone to see at a glance how reliable their friends are .\nThere is no censorship in this proposal . Facebook needn \u2019 t bend its algorithms to suppress posts from users with poor reliability markers : Every user could still post whatever they want , regardless of whether the facts of the stories they share are in dispute . People could choose to use social media the same way they do today , but now they \u2019 d have a choice whenever they encounter new information . They might glance at the reliability marker before nodding along with a friend \u2019 s provocative post , and they might think twice before passing on a weird story from a friend with a red reliability marker . Most important of all , a green reliability marker could become a valuable resource , something to put on the line only in extraordinary cases \u2014 just like a real-life reputation .\nThere \u2019 s technology behind this idea , but it \u2019 s technology that already exists . It \u2019 s aimed at assisting rather than algorithmically replacing the testimonial norms that have been regulating our information-gathering since long before social media came along . In the end , the solution for fake news won \u2019 t be just clever programming : it will also involve each of us taking up our responsibilities as digital citizens and putting our epistemic reputations on the line .\nRegina Rini ( @ rinireg ) teaches philosophy at York University in Toronto , where she holds the Canada Research Chair in Philosophy of Moral and Social Cognition .\nNow in print : \u201c Modern Ethics in 77 Arguments \u201d and \u201c The Stone Reader : Modern Philosophy in 133 Arguments , \u201d with essays from the series , edited by Peter Catapano and Simon Critchley , published by Liveright Books .\nFollow The New York Times Opinion section on Facebook and Twitter ( @ NYTopinion ) .",
    "content_original": "Here\u2019s a system that might help, and it is based on something that Facebook already does to prevent the spread of fake news. Currently, Facebook asks independent fact-checking organizations from across the political spectrum to identify false and misleading information. Whenever users try to post something that has been identified as fake news, they are confronted by a pop-up that explains the problems with the news and asks them to confirm if they\u2019d like to continue. None of these users are prevented from posting stories whose facts are in dispute, but they are required to know that what they are sharing may be false or misleading.\n\nFacebook has been openly using this system since December 2016. Less openly, they have also been keeping tabs on how often its users attempt to flag stories as fake news, and, using this feature, they have been calculating the epistemic reliability of their users. The Washington Post reported in August that Facebook secretly calculates scores that represent how often users\u2019 flags align with the analysis of independent fact-checkers. Facebook only uses this data internally, to identify abuse of the flagging system, and does not release it to users. I can\u2019t find out my own reputation score, or the scores of any of my friends.\n\nThis system and the secrecy around it may come across as a bit creepy \u2014 and the public trust in Facebook has been seriously and justifiably damaged \u2014 but I think that Facebook is on to something . Last year, in a paper published in the Kennedy Institute of Ethics Journal, I proposed a somewhat different system. The key difference between my system and the one that Facebook has implemented is transparency: Facebook should track and display how often each user decides to share disputed information after being warned that the information might be false or misleading.\n\nInstead of using this data to calculate a secret score, Facebook should display a simple reliability marker on every post and comment. Imagine a little colored dot next to the user\u2019s name, similar to the blue verification badges Facebook and Twitter give to trusted accounts: a green dot could indicate that the user hasn\u2019t chosen to share much disputed news, a yellow dot could indicate that they do it sometimes, and a red dot could indicate that they do it often. These reliability markers would allow anyone to see at a glance how reliable their friends are.\n\nThere is no censorship in this proposal. Facebook needn\u2019t bend its algorithms to suppress posts from users with poor reliability markers: Every user could still post whatever they want, regardless of whether the facts of the stories they share are in dispute. People could choose to use social media the same way they do today, but now they\u2019d have a choice whenever they encounter new information. They might glance at the reliability marker before nodding along with a friend\u2019s provocative post, and they might think twice before passing on a weird story from a friend with a red reliability marker. Most important of all, a green reliability marker could become a valuable resource, something to put on the line only in extraordinary cases \u2014 just like a real-life reputation.\n\nThere\u2019s technology behind this idea, but it\u2019s technology that already exists. It\u2019s aimed at assisting rather than algorithmically replacing the testimonial norms that have been regulating our information-gathering since long before social media came along. In the end, the solution for fake news won\u2019t be just clever programming: it will also involve each of us taking up our responsibilities as digital citizens and putting our epistemic reputations on the line.\n\nRegina Rini (@rinireg) teaches philosophy at York University in Toronto, where she holds the Canada Research Chair in Philosophy of Moral and Social Cognition.\n\nNow in print: \u201cModern Ethics in 77 Arguments\u201d and \u201cThe Stone Reader: Modern Philosophy in 133 Arguments,\u201d with essays from the series, edited by Peter Catapano and Simon Critchley, published by Liveright Books.\n\nFollow The New York Times Opinion section on Facebook and Twitter (@NYTopinion).",
    "source_url": "www.nytimes.com",
    "bias_text": "left",
    "ID": "FtYCXawUF99TjIDV"
}