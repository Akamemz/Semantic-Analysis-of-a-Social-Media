{
    "topic": "media_bias",
    "source": "Christian Science Monitor",
    "bias": 1,
    "url": "http://www.csmonitor.com/Technology/2016/0511/Is-Facebook-reinforcing-your-political-bias",
    "title": "Is Facebook reinforcing your political bias?",
    "date": "2016-05-11",
    "authors": "Max Lewontin",
    "content": "At the 2006 White House Correspondents ' Dinner , host Stephen Colbert famously asserted that \u201c reality has a well known liberal bias . \u201d\nHis claim was in jest , but a former Facebook employee \u2019 s contention that the site \u2019 s \u201c news curators \u201d routinely omitted popular conservative news from its \u201c trending news \u201d feed has reignited a long-running debate about online news , media bias , and what political scientists say is a trend toward increasing political polarization .\nFor what 's increasingly a primary news source for its 1 billion daily users , Facebook could be a significant influence on what is considered true in a US election year .\nIn a report published Monday by the tech site Gizmodo , the employee alleged that news stories featured on Facebook were selected from a small pool of trusted news sources , such as The New York Times , the BBC , or the Guardian , at the exclusion of several others .\nFacebook has denied the report , saying it doesn \u2019 t censor particular articles and enforces \u201c rigorous guidelines \u201d to bar reviewers from doing so .\n\u201c I don \u2019 t know where that \u2019 s coming from , \u201d a Facebook spokesperson tells The \u2588\u2588\u2588 .\nThe company has faced questions about its influence on politics in the past \u2013 comments by chief executive Mark Zuckerberg aimed at Donald Trump led to speculation that the site would seek to influence the 2016 election , while a tweet from a Facebook board member that appeared to endorse colonialism in India became part of a movement to bar its Free Basics site from the country .\nThe allegations about the news curators , who were described by Gizmodo as a \u201c small group of young journalists , primarily educated at Ivy League or private East Coast universities \u201d \u2013 could further challenge the site \u2019 s longstanding claims of technological neutrality .\n\u201c I was really surprised , \u201d says Jason Gainous , a professor of political science at the University of Louisville . \u201c I hadn \u2019 t even thought about that possibility . I know their algorithm filters out based on user preferences but the idea that they \u2019 re actually filtering out their trending stories , this is not good news for them . \u201d\nIf it is occurring , such filtering could potentially alter the views of conservative users , some say .\n\u201c People tend to select information matching their political beliefs . If Facebook were systematically favoring one political perspective over another , then it would challenge this trend for those on one side of the political aisle , \u201d writes Natalie Jomini Stroud , an associate professor of communication at the University of Texas at Austin who directs the Engaging News Project , in an e-mail to the Monitor .\nThe former Facebook news curator \u2019 s claim , which was contested by other curators interviewed by Gizmodo and The Guardian , sparked a firestorm of criticism from some conservative news sites . But the growing polarization of our news consumption may not require help from social media . Instead , it may be an outgrowth of the manner in which we consume our news , experts say .\nWith trust in government peaking in the mid-1960s and a decline in belief in established information sources , including the news media , many Americans have increasingly become polarized in their political views and self-selected into like-minded communities , says Bill Bishop , a journalist and author of \u201c The Big Sort : Why the Clustering of Like-Minded America is Tearing Us Apart . \u201d\nThat `` clustering '' tendency may be further enabled by social networking sites , which continue to usurp broadcast news and newspapers as a key central destination for news . But there are some distinctions in how users seek out news online on different platforms .\nA study from the Pew Research Center found that more than half of users of both Facebook and Twitter used the platforms as a news source for events beyond their friends and family .\nBut while Twitter is seen primarily as a tool for keeping up with breaking news and following their favorite outlets , reporters , and commentators , Facebook functions more as a forum . Its users were more likely to post and respond to content about government and politics .\nCould trending news stories actually impact users \u2019 political views ? It \u2019 s still hard to tell .\n\u201c There is research suggesting that those selecting like-minded partisan media hold more polarized political views . It \u2019 s not clear to me whether the \u2018 Trending \u2019 feature would have the same effect , \u201d writes Stroud , the communication professor in Texas . `` What may be more likely is that the \u2018 Trending \u2019 feature influences what issues people believe are most important , \u201d she says .\nAccusations of bias could be worsened by the fact that Facebook \u2019 s news feeds are lightly tailored . The trending feed also has some differences from what users see on their personal news feed , the Facebook spokesperson says .\nTrending topics are generated through what users are talking about on the site , then \u201c lightly curated \u201d by Facebook \u2019 s review team , the company 's spokesperson tells the Monitor .\n\u201c Popular topics are first surfaced by an algorithm , then audited by review team members to confirm that the topics are in fact trending news in the real world and not , for example , similar-sounding topics or misnomers , \u201d writes Tom Stocky , Facebook \u2019 s vice president of search , in a post on the site on Monday .\nMr. Stocky also disputes a contention that the news curators artificially \u201c injected \u201d stories into the trending feed , including adding stories about the civil rights movement # BlackLivesMatter when they were not trending .\n\u201c Facebook does not allow or advise our reviewers to systematically discriminate against sources of any ideological origin and we 've designed our tools to make that technically not feasible . At the same time , our reviewers ' actions are logged and reviewed , and violating our guidelines is a fireable offense , \u201d he writes .\nInstead , Facebook has argued that the types of stories that people see on the site are based mostly on who users \u2019 friends are and what they share , not the site \u2019 s algorithm .\nUsing data from more than 10 million users , researchers from the company found the site \u2019 s algorithm reduces so-called cross-cutting material \u2013 or content that runs counter to a user \u2019 s own political views \u2013 by slightly less than 1 percent . A user \u2019 s own \u201c filter bubble \u201d of friends , by contrast , reduces such content by about 4 percent .\nBy design , Facebook encourages users to self-select , with political views playing a key role , says Mr. Bishop .\n\u201c They \u2019 ve built a site that is profitable because it caters to people \u2019 s need to self-express and curate and refine their images and individual brands , and they do that within groups where they feel comfortable because everyone is like them . It \u2019 s the site for our time , \u201d he says .\nAdditionally , some users are making conscious decisions to attempt to influence what types of content will appear in their own news feeds .\nSeveral \u201c folk theories \u201d \u2013 including a \u201c Narcissus Theory \u201d that users will see more from friends similar to them and a perspective that suggests Facebook is all powerful and unknowable \u2013 shaped how some users manipulated the site , says Karrie Karahalios , an associate professor of computer science at the University of Illinois at Urbana-Champaign .\nDr. Karahalios and several colleagues collected these folk theories together in a recently published paper by giving users access to an interface disclosing \u201c seams \u201d that provided hints into how Facebook \u2019 s algorithm works .\nGet the Monitor Stories you care about delivered to your inbox . By signing up , you agree to our Privacy Policy\n\u201c We found that it got people thinking a little bit more and it got them to try things on Facebook that they wouldn \u2019 t have thought of before , they had a bit more knowledge and they had a tool set available to them that they could put action into their news feed , \u201d she says .",
    "content_original": "At the 2006 White House Correspondents' Dinner, host Stephen Colbert famously asserted that \u201creality has a well known liberal bias.\u201d\n\nHis claim was in jest, but a former Facebook employee\u2019s contention that the site\u2019s \u201cnews curators\u201d routinely omitted popular conservative news from its \u201ctrending news\u201d feed has reignited a long-running debate about online news, media bias, and what political scientists say is a trend toward increasing political polarization.\n\nFor what's increasingly a primary news source for its 1 billion daily users, Facebook could be a significant influence on what is considered true in a US election year.\n\nIn a report published Monday by the tech site Gizmodo, the employee alleged that news stories featured on Facebook were selected from a small pool of trusted news sources, such as The New York Times, the BBC, or the Guardian, at the exclusion of several others.\n\nFacebook has denied the report, saying it doesn\u2019t censor particular articles and enforces \u201crigorous guidelines\u201d to bar reviewers from doing so.\n\n\u201cI don\u2019t know where that\u2019s coming from,\u201d a Facebook spokesperson tells The Christian Science Monitor.\n\nThe company has faced questions about its influence on politics in the past \u2013 comments by chief executive Mark Zuckerberg aimed at Donald Trump led to speculation that the site would seek to influence the 2016 election, while a tweet from a Facebook board member that appeared to endorse colonialism in India became part of a movement to bar its Free Basics site from the country.\n\nThe allegations about the news curators, who were described by Gizmodo as a \u201csmall group of young journalists, primarily educated at Ivy League or private East Coast universities\u201d \u2013 could further challenge the site\u2019s longstanding claims of technological neutrality.\n\n\"Leaning Left\"?\n\n\u201cI was really surprised,\u201d says Jason Gainous, a professor of political science at the University of Louisville. \u201cI hadn\u2019t even thought about that possibility. I know their algorithm filters out based on user preferences but the idea that they\u2019re actually filtering out their trending stories, this is not good news for them.\u201d\n\nIf it is occurring, such filtering could potentially alter the views of conservative users, some say.\n\n\u201cPeople tend to select information matching their political beliefs. If Facebook were systematically favoring one political perspective over another, then it would challenge this trend for those on one side of the political aisle,\u201d writes Natalie Jomini Stroud, an associate professor of communication at the University of Texas at Austin who directs the Engaging News Project, in an e-mail to the Monitor.\n\nThe former Facebook news curator\u2019s claim, which was contested by other curators interviewed by Gizmodo and The Guardian, sparked a firestorm of criticism from some conservative news sites. But the growing polarization of our news consumption may not require help from social media. Instead, it may be an outgrowth of the manner in which we consume our news, experts say.\n\nWith trust in government peaking in the mid-1960s and a decline in belief in established information sources, including the news media, many Americans have increasingly become polarized in their political views and self-selected into like-minded communities, says Bill Bishop, a journalist and author of \u201cThe Big Sort: Why the Clustering of Like-Minded America is Tearing Us Apart.\u201d\n\nIncreasing dominance of online news\n\nThat \"clustering\" tendency may be further enabled by social networking sites, which continue to usurp broadcast news and newspapers as a key central destination for news. But there are some distinctions in how users seek out news online on different platforms.\n\nA study from the Pew Research Center found that more than half of users of both Facebook and Twitter used the platforms as a news source for events beyond their friends and family.\n\nBut while Twitter is seen primarily as a tool for keeping up with breaking news and following their favorite outlets, reporters, and commentators, Facebook functions more as a forum. Its users were more likely to post and respond to content about government and politics.\n\nCould trending news stories actually impact users\u2019 political views? It\u2019s still hard to tell.\n\n\u201cThere is research suggesting that those selecting like-minded partisan media hold more polarized political views. It\u2019s not clear to me whether the \u2018Trending\u2019 feature would have the same effect,\u201d writes Stroud, the communication professor in Texas. \"What may be more likely is that the \u2018Trending\u2019 feature influences what issues people believe are most important,\u201d she says.\n\nGaming the news feed, or just personal preference?\n\nAccusations of bias could be worsened by the fact that Facebook\u2019s news feeds are lightly tailored. The trending feed also has some differences from what users see on their personal news feed, the Facebook spokesperson says.\n\nTrending topics are generated through what users are talking about on the site, then \u201clightly curated\u201d by Facebook\u2019s review team, the company's spokesperson tells the Monitor.\n\n\u201cPopular topics are first surfaced by an algorithm, then audited by review team members to confirm that the topics are in fact trending news in the real world and not, for example, similar-sounding topics or misnomers,\u201d writes Tom Stocky, Facebook\u2019s vice president of search, in a post on the site on Monday.\n\nMr. Stocky also disputes a contention that the news curators artificially \u201cinjected\u201d stories into the trending feed, including adding stories about the civil rights movement #BlackLivesMatter when they were not trending.\n\n\u201cFacebook does not allow or advise our reviewers to systematically discriminate against sources of any ideological origin and we've designed our tools to make that technically not feasible. At the same time, our reviewers' actions are logged and reviewed, and violating our guidelines is a fireable offense,\u201d he writes.\n\nInstead, Facebook has argued that the types of stories that people see on the site are based mostly on who users\u2019 friends are and what they share, not the site\u2019s algorithm.\n\nUsing data from more than 10 million users, researchers from the company found the site\u2019s algorithm reduces so-called cross-cutting material \u2013 or content that runs counter to a user\u2019s own political views \u2013 by slightly less than 1 percent. A user\u2019s own \u201cfilter bubble\u201d of friends, by contrast, reduces such content by about 4 percent.\n\nBy design, Facebook encourages users to self-select, with political views playing a key role, says Mr. Bishop.\n\n\u201cThey\u2019ve built a site that is profitable because it caters to people\u2019s need to self-express and curate and refine their images and individual brands, and they do that within groups where they feel comfortable because everyone is like them. It\u2019s the site for our time,\u201d he says.\n\nAdditionally, some users are making conscious decisions to attempt to influence what types of content will appear in their own news feeds.\n\nSeveral \u201cfolk theories\u201d \u2013 including a \u201cNarcissus Theory\u201d that users will see more from friends similar to them and a perspective that suggests Facebook is all powerful and unknowable \u2013 shaped how some users manipulated the site, says Karrie Karahalios, an associate professor of computer science at the University of Illinois at Urbana-Champaign.\n\nDr. Karahalios and several colleagues collected these folk theories together in a recently published paper by giving users access to an interface disclosing \u201cseams\u201d that provided hints into how Facebook\u2019s algorithm works.\n\nGet the Monitor Stories you care about delivered to your inbox. By signing up, you agree to our Privacy Policy\n\n\u201cWe found that it got people thinking a little bit more and it got them to try things on Facebook that they wouldn\u2019t have thought of before, they had a bit more knowledge and they had a tool set available to them that they could put action into their news feed,\u201d she says.\n\nEditor's note: This article originally misstated the title of Jason Gainous at the University of Louisville.",
    "source_url": "www.csmonitor.com",
    "bias_text": "center",
    "ID": "rPkc6gsEtkBEM45B"
}